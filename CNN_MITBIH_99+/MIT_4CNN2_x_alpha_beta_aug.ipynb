{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import time\n",
    "plc=7\n",
    "time.sleep(1000*plc)         #run after 1 houre = 3600 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#notebook { padding-top:0px !important; } .container { width:100% !important; } .end_space { min-height:0px !important; } html, body, .container{ margin:0!important;padding:0!important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import scipy\n",
    "import random\n",
    "import bisect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess import *                         #ماژول استخراج پنجره ها\n",
    "from data import *                          #ماژول محلی ورود داده ها\n",
    "from augment import *                            #ماژول های داده افزایی\n",
    "from lstm_cnn import *                           #ماژول های داده افزایی با lstm_cnn\n",
    "import augment\n",
    "import importlib\n",
    "importlib.reload(augment)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,recall_score,precision_score\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D,Conv1D,Dropout,MaxPooling1D,MaxPooling2D,Flatten,Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "cwd = os.getcwd() #\n",
    "fullscrn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wndwng0d(x,snstvty=2):            #روش تشخیص ایپاک پایه\n",
    "    smpl_rte=480\n",
    "    i=0\n",
    "    wndws=np.array([])\n",
    "    #for i in range (1,2):   #print(np.shape(x))\n",
    "    if snstvty==1 :                        #تشخیص ایپاک پایه\n",
    "        xj=np.array(x)\n",
    "        peaks,_ = find_peaks(xj, height=0.03*max(xj), distance=200,prominence=300, width=3)\n",
    "        if len(peaks)<6 :\n",
    "            peaks,_ = find_peaks(xj, height=0.03*max(xj), distance=160,prominence=100, width=3)\n",
    "            #print('class ', i, 'Row = ', j, 'have ',len(peaks),'peaks')\n",
    "        if len(peaks)>1 :\n",
    "            for z in range (len(peaks)):\n",
    "                if (peaks[z]<3100 and peaks[z]!=peaks[-1]) :\n",
    "                    win=np.array(xj[peaks[z]:peaks[z+1]])\n",
    "                    win = signal.resample(win, smpl_rte)\n",
    "                    if len(win)==smpl_rte :  #win=np.append(win, i)# if len(win)==smpl_rte+1 :\n",
    "                        wndws=np.append(wndws, win)\n",
    "\n",
    "\n",
    "    if snstvty==2 :                        #تشخیص ایپاک حساس تر\n",
    "        xj=np.array(x)\n",
    "        peaks,_ = find_peaks(xj, height=0.02*max(xj), distance=160,prominence=200, width=3)\n",
    "        if len(peaks)<6 :\n",
    "            peaks,_ = find_peaks(xj, height=0.01*max(xj), distance=120,prominence=60, width=3)\n",
    "        if len(peaks)>1 :\n",
    "            for z in range (len(peaks)):\n",
    "                if (peaks[z]<3100 and peaks[z]!=peaks[-1]) :\n",
    "                    win=np.array(xj[peaks[z]:peaks[z+1]])\n",
    "                    win = signal.resample(win, smpl_rte)\n",
    "                    if len(win)==smpl_rte :\n",
    "                        wndws=np.append(wndws, win)\n",
    "    rows=int(len(wndws)/(smpl_rte))            #/(smpl_rte+1))\n",
    "    wndws0=np.reshape(wndws,(rows,(smpl_rte)))    #(wndws,(rows,(smpl_rte+1)))\n",
    "    return(wndws0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><div style=\"direction:rtl;font-family:B Nazanin\">Importing Data</div></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 283\t2 66\t3 20\t4 135\t5 13\t6 21\t7 133\t8 55\t9 13\t10 10\t11 10\t12 10\t13 11\t14 103\t15 62\t16 10\t17 45\t"
     ]
    }
   ],
   "source": [
    "cls_num=17\n",
    "for i in range (1,18):\n",
    "    vars()['ecg'+str(i)]=MITBIH(i)\n",
    "os.chdir(cwd)\n",
    "clses_lens=np.array([])\n",
    "i=0                               #جمع آوری داده ها و چاپ تعداد نمونه ی هر کلاس\n",
    "ecg=np.array(ecg1)\n",
    "print(1,len(vars()['ecg'+str(1)]), end='\\t')\n",
    "clses_lens=np.append(clses_lens,len(vars()['ecg'+str(1)]))\n",
    "for i in range (2,18):\n",
    "    ecg=np.concatenate((ecg,vars()['ecg'+str(i)]),axis=0)\n",
    "    clses_lens=np.append(clses_lens,len(vars()['ecg'+str(i)]))\n",
    "    print( i,len(vars()['ecg'+str(i)]), end='\\t')\n",
    "\n",
    "mx_sig=max(clses_lens)\n",
    "btch=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center><div style=\"direction:rtl;font-family:B Nazanin\">Base Train windows</div></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each class and its windows =\n",
      "1 2598\t2 788\t3 224\t4 1564\t5 158\t6 189\t7 1371\t8 553\t9 114\t10 104\t11 72\t12 107\t13 156\t14 1014\t15 633\t16 45\t17 450\t\n",
      " max = 2598\n"
     ]
    }
   ],
   "source": [
    "smpl_rte=480                                              # در ماژول ها نیز همین مقدار ثبت شده\n",
    "i=0\n",
    "windws=np.array([])\n",
    "for cls in range (1,18):                                 #ساخت پنجره های داده های آموزش اصلی\n",
    "    dta=np.array(vars()['ecg'+str(cls)][int(.1*len(vars()['ecg'+str(cls)])):,:]) #انتخاب محدوده ترِین جهت داده افزایی\n",
    "    vars()['wndws'+str(cls)]=wndwng(dta)                #نرخ نمونه برداری 480 است و شماره کلاس در آخر ردیف نیست\n",
    "    clm0vlu=np.zeros(len(vars()['wndws'+str(cls)]))\n",
    "    cls_clm=np.array(clm0vlu)\n",
    "    cls_clm=np.int16(cls_clm+cls)\n",
    "    cls_clm=np.transpose([cls_clm])                     # افزودن ستون برچسب (شماره کلاس) به سمت راست پنجره ها\n",
    "    vars()['wndws'+str(cls)]=np.concatenate((vars()['wndws'+str(cls)],cls_clm),axis=1)   \n",
    "\n",
    "print(\"each class and its windows =\")\n",
    "cls_wndws=np.array([])\n",
    "for cls in range (1,18):                                # آرایه ی تعداد پنجره ی هر کلاس\n",
    "    wns=len(vars()['wndws'+str(cls)])\n",
    "    cls_wndws=np.append(cls_wndws,wns)\n",
    "    print(cls, wns, end='\\t')\n",
    "cls_wndws=np.int16(cls_wndws)    \n",
    "mx_wndws=int(np.max(cls_wndws))                        # حداکثر تعداد پنجره ی موجود بین کلاس ها\n",
    "print('\\n max =', mx_wndws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><div style=\"direction:rtl;font-family:B Nazanin\">Data Augmentation</div></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " max = 311\n",
      "x= 1 alpha= 1 beta= 1\n",
      "[[ 20  41  44  41  12   7  41]\n",
      " [ 49 104 110 104  31  18 104]\n",
      " [ 34  72  76  72  21  13  72]\n",
      " [ 24  50  53  50  15   9  50]\n",
      " [ 55 116 123 116  34  20 116]\n",
      " [ 34  72  77  72  21  13  72]\n",
      " [ 37  78  83  78  23  14  78]\n",
      " [ 28  59  62  59  17  10  59]\n",
      " [ 40  86  91  86  25  15  86]\n",
      " [ 55 118 125 118  35  21 118]\n",
      " [ 45  96 101  96  28  17  96]\n",
      " [ 33  70  74  70  21  12  70]\n",
      " [ 56 118 125 118  35  21 118]\n",
      " [ 17  36  38  36  11   6  36]\n",
      " [ 23  49  52  49  14   9  49]\n",
      " [ 29  62  65  62  18  11  62]\n",
      " [ 25  54  57  54  16   9  54]]\n",
      "x= 1 alpha= 1 beta= 2\n",
      "[[  7  33  38  33   3   1  33]\n",
      " [ 25 114 128 114  10   4 114]\n",
      " [ 15  67  75  67   6   2  67]\n",
      " [  9  42  47  42   4   1  42]\n",
      " [ 29 130 146 130  11   4 130]\n",
      " [ 15  69  77  69   6   2  69]\n",
      " [ 18  79  89  79   7   2  79]\n",
      " [ 13  58  65  58   5   2  58]\n",
      " [ 18  80  90  80   7   2  80]\n",
      " [ 29 132 148 132  11   4 132]\n",
      " [ 20  92 103  92   8   3  92]\n",
      " [ 15  68  77  68   6   2  68]\n",
      " [ 29 132 148 132  11   4 132]\n",
      " [  9  39  44  39   3   1  39]\n",
      " [ 12  54  61  54   5   2  54]\n",
      " [ 15  68  77  68   6   2  68]\n",
      " [ 13  60  67  60   5   2  60]]\n",
      "x= 1 alpha= 1 beta= 6\n",
      "[[  0   9  13   9   0   0   9]\n",
      " [  1 106 149 106   0   0 106]\n",
      " [  1  62  88  62   0   0  62]\n",
      " [  0  36  50  36   0   0  36]\n",
      " [  1 131 185 131   0   0 131]\n",
      " [  1  66  92  66   0   0  66]\n",
      " [  1  57  80  57   0   0  57]\n",
      " [  1  57  81  57   0   0  57]\n",
      " [  1  69  97  69   0   0  69]\n",
      " [  1 133 188 133   0   0 133]\n",
      " [  1  72 102  72   0   0  72]\n",
      " [  1  67  94  67   0   0  67]\n",
      " [  1 133 188 133   0   0 133]\n",
      " [  0  40  56  40   0   0  40]\n",
      " [  1  55  77  55   0   0  55]\n",
      " [  1  69  97  69   0   0  69]\n",
      " [  1  60  85  60   0   0  60]]\n",
      "x= 1 alpha= 2 beta= 1\n",
      "[[ 39  83  88  83  24  15  83]\n",
      " [ 75 159 169 159  47  28 159]\n",
      " [ 42  89  94  89  26  16  89]\n",
      " [ 33  69  73  69  20  12  69]\n",
      " [ 82 174 184 174  51  31 174]\n",
      " [ 41  87  92  87  26  15  87]\n",
      " [ 60 127 134 127  37  22 127]\n",
      " [ 31  67  71  67  20  12  67]\n",
      " [ 52 111 117 111  33  20 111]\n",
      " [ 83 175 186 175  52  31 175]\n",
      " [ 61 130 138 130  38  23 130]\n",
      " [ 38  81  85  81  24  14  81]\n",
      " [ 83 176 186 176  52  31 176]\n",
      " [ 17  36  38  36  11   6  36]\n",
      " [ 23  50  53  50  15   9  50]\n",
      " [ 29  62  66  62  18  11  62]\n",
      " [ 26  54  58  54  16  10  54]]\n",
      "x= 1 alpha= 2 beta= 2\n",
      "[[ 15  67  75  67   6   2  67]\n",
      " [ 38 173 194 173  15   5 173]\n",
      " [ 16  73  82  73   6   2  73]\n",
      " [ 11  49  55  49   4   2  49]\n",
      " [ 43 195 218 195  17   6 195]\n",
      " [ 16  73  82  73   6   2  73]\n",
      " [ 28 125 140 125  11   4 125]\n",
      " [ 13  59  66  59   5   2  59]\n",
      " [ 20  92 104  92   8   3  92]\n",
      " [ 44 197 220 197  17   6 197]\n",
      " [ 25 115 129 115  10   4 115]\n",
      " [ 16  71  79  71   6   2  71]\n",
      " [ 44 197 221 197  17   6 197]\n",
      " [  9  39  44  39   3   1  39]\n",
      " [ 12  54  61  54   5   2  54]\n",
      " [ 15  68  77  68   6   2  68]\n",
      " [ 13  60  67  60   5   2  60]]\n",
      "x= 1 alpha= 2 beta= 6\n",
      "[[  0  18  26  18   0   0  18]\n",
      " [  2 157 221 157   0   0 157]\n",
      " [  1  63  88  63   0   0  63]\n",
      " [  0  36  50  36   0   0  36]\n",
      " [  2 196 277 196   0   0 196]\n",
      " [  1  66  92  66   0   0  66]\n",
      " [  1  80 112  80   0   0  80]\n",
      " [  1  57  81  57   0   0  57]\n",
      " [  1  69  97  69   0   0  69]\n",
      " [  2 198 280 198   0   0 198]\n",
      " [  1  75 106  75   0   0  75]\n",
      " [  1  67  94  67   0   0  67]\n",
      " [  2 199 280 199   0   0 199]\n",
      " [  0  40  56  40   0   0  40]\n",
      " [  1  55  77  55   0   0  55]\n",
      " [  1  69  97  69   0   0  69]\n",
      " [  1  60  85  60   0   0  60]]\n",
      "x= 1 alpha= 3 beta= 1\n",
      "[[ 59 124 132 124  37  22 124]\n",
      " [101 214 227 214  63  38 214]\n",
      " [ 50 105 111 105  31  19 105]\n",
      " [ 42  88  94  88  26  16  88]\n",
      " [109 231 245 231  68  41 231]\n",
      " [ 48 101 107 101  30  18 101]\n",
      " [ 82 175 185 175  52  31 175]\n",
      " [ 35  75  79  75  22  13  75]\n",
      " [ 64 136 144 136  40  24 136]\n",
      " [110 233 247 233  69  41 233]\n",
      " [ 77 165 174 165  48  29 165]\n",
      " [ 43  92  97  92  27  16  92]\n",
      " [110 233 247 233  69  41 233]\n",
      " [ 17  37  39  37  11   7  37]\n",
      " [ 24  50  53  50  15   9  50]\n",
      " [ 30  63  66  63  18  11  63]\n",
      " [ 26  55  58  55  16  10  55]]\n",
      "x= 1 alpha= 3 beta= 2\n",
      "[[ 22 100 113 100   9   3 100]\n",
      " [ 52 233 261 233  20   7 233]\n",
      " [ 17  78  88  78   7   2  78]\n",
      " [ 12  56  63  56   5   2  56]\n",
      " [ 57 259 291 259  22   8 259]\n",
      " [ 17  77  86  77   7   2  77]\n",
      " [ 38 170 191 170  15   5 170]\n",
      " [ 13  61  68  61   5   2  61]\n",
      " [ 23 105 118 105   9   3 105]\n",
      " [ 58 261 293 261  23   8 261]\n",
      " [ 31 138 155 138  12   4 138]\n",
      " [ 16  73  82  73   6   2  73]\n",
      " [ 58 261 293 261  23   8 261]\n",
      " [  9  39  44  39   3   1  39]\n",
      " [ 12  55  61  55   5   2  55]\n",
      " [ 15  68  77  68   6   2  68]\n",
      " [ 13  60  67  60   5   2  60]]\n",
      "x= 1 alpha= 3 beta= 6\n",
      "[[  0  27  38  27   0   0  27]\n",
      " [  2 208 293 208   0   0 208]\n",
      " [  1  63  88  63   0   0  63]\n",
      " [  0  36  50  36   0   0  36]\n",
      " [  3 261 368 261   0   0 261]\n",
      " [  1  66  93  66   0   0  66]\n",
      " [  1 103 145 103   0   0 103]\n",
      " [  1  57  81  57   0   0  57]\n",
      " [  1  70  98  70   0   0  70]\n",
      " [  3 264 371 264   0   0 264]\n",
      " [  1  78 110  78   0   0  78]\n",
      " [  1  67  94  67   0   0  67]\n",
      " [  3 264 372 264   0   0 264]\n",
      " [  0  40  56  40   0   0  40]\n",
      " [  1  55  77  55   0   0  55]\n",
      " [  1  69  97  69   0   0  69]\n",
      " [  1  60  85  60   0   0  60]]\n",
      "x= 2 alpha= 1 beta= 1\n",
      "[[ 68 145 153 145  43  26 145]\n",
      " [104 221 234 221  65  39 221]\n",
      " [ 71 151 159 151  44  27 151]\n",
      " [ 62 131 139 131  39  23 131]\n",
      " [111 236 249 236  69  42 236]\n",
      " [ 70 149 158 149  44  26 149]\n",
      " [ 89 189 200 189  56  33 189]\n",
      " [ 61 129 136 129  38  23 129]\n",
      " [ 81 173 183 173  51  31 173]\n",
      " [112 237 251 237  70  42 237]\n",
      " [ 90 192 203 192  57  34 192]\n",
      " [ 67 143 151 143  42  25 143]\n",
      " [112 238 252 238  70  42 238]\n",
      " [ 46  98 104  98  29  17  98]\n",
      " [ 53 112 118 112  33  20 112]\n",
      " [ 58 124 131 124  37  22 124]\n",
      " [ 55 116 123 116  34  21 116]]\n",
      "x= 2 alpha= 1 beta= 2\n",
      "[[ 30 136 153 136  12   4 136]\n",
      " [ 54 243 272 243  21   8 243]\n",
      " [ 32 142 159 142  12   4 142]\n",
      " [ 26 119 133 119  10   4 119]\n",
      " [ 58 264 296 264  23   8 264]\n",
      " [ 32 143 160 143  12   4 143]\n",
      " [ 43 194 218 194  17   6 194]\n",
      " [ 29 129 144 129  11   4 129]\n",
      " [ 36 162 182 162  14   5 162]\n",
      " [ 59 266 298 266  23   8 266]\n",
      " [ 41 184 207 184  16   6 184]\n",
      " [ 31 140 157 140  12   4 140]\n",
      " [ 59 266 299 266  23   8 266]\n",
      " [ 24 109 122 109   9   3 109]\n",
      " [ 27 124 139 124  11   4 124]\n",
      " [ 31 138 155 138  12   4 138]\n",
      " [ 29 129 145 129  11   4 129]]\n",
      "x= 2 alpha= 1 beta= 6\n",
      "[[  1  88 124  88   0   0  88]\n",
      " [  2 227 320 227   0   0 227]\n",
      " [  1 133 187 133   0   0 133]\n",
      " [  1 106 149 106   0   0 106]\n",
      " [  3 266 375 266   0   0 266]\n",
      " [  1 136 191 136   0   0 136]\n",
      " [  2 150 211 150   0   0 150]\n",
      " [  1 127 179 127   0   0 127]\n",
      " [  2 139 196 139   0   0 139]\n",
      " [  3 268 378 268   0   0 268]\n",
      " [  2 145 205 145   0   0 145]\n",
      " [  1 137 193 137   0   0 137]\n",
      " [  3 269 379 269   0   0 269]\n",
      " [  1 110 155 110   0   0 110]\n",
      " [  1 125 176 125   0   0 125]\n",
      " [  2 139 196 139   0   0 139]\n",
      " [  1 130 184 130   0   0 130]]\n",
      "x= 2 alpha= 2 beta= 1\n",
      "[[107 228 241 228  67  40 228]\n",
      " [156 332 351 332  98  59 332]\n",
      " [ 87 184 195 184  54  32 184]\n",
      " [ 80 169 179 169  50  30 169]\n",
      " [165 351 371 351 103  62 351]\n",
      " [ 84 178 188 178  52  31 178]\n",
      " [134 286 302 286  84  50 286]\n",
      " [ 68 145 153 145  43  26 145]\n",
      " [105 224 237 224  66  39 224]\n",
      " [166 353 373 353 104  62 353]\n",
      " [123 261 277 261  77  46 261]\n",
      " [ 77 165 174 165  48  29 165]\n",
      " [166 353 374 353 104  62 353]\n",
      " [ 47 100 105 100  29  18 100]\n",
      " [ 53 113 120 113  33  20 113]\n",
      " [ 59 125 133 125  37  22 125]\n",
      " [ 55 118 124 118  35  21 118]]\n",
      "x= 2 alpha= 2 beta= 2\n",
      "[[ 45 203 228 203  18   6 203]\n",
      " [ 80 362 406 362  31  11 362]\n",
      " [ 34 153 172 153  13   5 153]\n",
      " [ 29 133 149 133  11   4 133]\n",
      " [ 87 393 441 393  34  12 393]\n",
      " [ 33 151 169 151  13   5 151]\n",
      " [ 63 285 320 285  25   9 285]\n",
      " [ 29 131 147 131  11   4 131]\n",
      " [ 41 187 210 187  16   6 187]\n",
      " [ 88 395 443 395  34  12 395]\n",
      " [ 51 231 259 231  20   7 231]\n",
      " [ 32 145 162 145  13   5 145]\n",
      " [ 88 395 443 395  34  12 395]\n",
      " [ 24 109 122 109   9   3 109]\n",
      " [ 27 124 139 124  11   4 124]\n",
      " [ 31 138 155 138  12   4 138]\n",
      " [ 29 129 145 129  11   4 129]]\n",
      "x= 2 alpha= 2 beta= 6\n",
      "[[  1 106 150 106   0   0 106]\n",
      " [  4 329 464 329   0   0 329]\n",
      " [  1 133 187 133   0   0 133]\n",
      " [  1 106 149 106   0   0 106]\n",
      " [  4 397 559 397   0   0 397]\n",
      " [  1 136 191 136   0   0 136]\n",
      " [  2 196 276 196   0   0 196]\n",
      " [  1 127 179 127   0   0 127]\n",
      " [  2 140 197 140   0   0 140]\n",
      " [  4 399 562 399   0   0 399]\n",
      " [  2 151 213 151   0   0 151]\n",
      " [  1 137 193 137   0   0 137]\n",
      " [  4 399 562 399   0   0 399]\n",
      " [  1 110 155 110   0   0 110]\n",
      " [  1 125 176 125   0   0 125]\n",
      " [  2 139 196 139   0   0 139]\n",
      " [  1 130 184 130   0   0 130]]\n",
      "x= 2 alpha= 3 beta= 1\n",
      "[[146 311 329 311  91  55 311]\n",
      " [208 442 468 442 130  78 442]\n",
      " [102 217 230 217  64  38 217]\n",
      " [ 98 207 220 207  61  37 207]\n",
      " [219 466 493 466 137  82 466]\n",
      " [ 97 206 219 206  61  36 206]\n",
      " [180 382 405 382 112  67 382]\n",
      " [ 76 161 170 161  47  28 161]\n",
      " [129 274 290 274  81  48 274]\n",
      " [220 468 495 468 138  83 468]\n",
      " [155 330 350 330  97  58 330]\n",
      " [ 88 186 197 186  55  33 186]\n",
      " [220 468 495 468 138  83 468]\n",
      " [ 47 101 107 101  30  18 101]\n",
      " [ 54 114 121 114  34  20 114]\n",
      " [ 60 126 134 126  37  22 126]\n",
      " [ 56 119 126 119  35  21 119]]\n",
      "x= 2 alpha= 3 beta= 2\n",
      "[[ 60 270 303 270  23   8 270]\n",
      " [107 481 539 481  42  15 481]\n",
      " [ 36 164 184 164  14   5 164]\n",
      " [ 33 147 165 147  13   5 147]\n",
      " [116 522 586 522  45  16 522]\n",
      " [ 35 159 178 159  14   5 159]\n",
      " [ 83 376 422 376  33  12 376]\n",
      " [ 30 134 150 134  12   4 134]\n",
      " [ 47 212 238 212  18   7 212]\n",
      " [116 524 588 524  45  16 524]\n",
      " [ 61 277 311 277  24   9 277]\n",
      " [ 33 149 167 149  13   5 149]\n",
      " [116 525 588 525  45  16 525]\n",
      " [ 24 109 122 109   9   3 109]\n",
      " [ 27 124 139 124  11   4 124]\n",
      " [ 31 138 155 138  12   4 138]\n",
      " [ 29 129 145 129  11   4 129]]\n",
      "x= 2 alpha= 3 beta= 6\n",
      "[[  1 125 176 125   0   0 125]\n",
      " [  5 431 607 431   0   0 431]\n",
      " [  1 133 187 133   0   0 133]\n",
      " [  1 106 150 106   0   0 106]\n",
      " [  6 527 743 527   0   0 527]\n",
      " [  1 136 191 136   0   0 136]\n",
      " [  3 241 340 241   0   0 241]\n",
      " [  1 127 179 127   0   0 127]\n",
      " [  2 141 199 141   0   0 141]\n",
      " [  6 529 745 529   0   0 529]\n",
      " [  2 157 222 157   0   0 157]\n",
      " [  1 137 193 137   0   0 137]\n",
      " [  6 529 746 529   0   0 529]\n",
      " [  1 110 155 110   0   0 110]\n",
      " [  1 125 176 125   0   0 125]\n",
      " [  2 139 196 139   0   0 139]\n",
      " [  1 130 184 130   0   0 130]]\n",
      "x= 3 alpha= 1 beta= 1\n",
      "[[117 248 263 248  73  44 248]\n",
      " [159 338 358 338 100  60 338]\n",
      " [108 229 243 229  67  40 229]\n",
      " [100 212 225 212  62  37 212]\n",
      " [167 355 376 355 104  63 355]\n",
      " [106 225 238 225  66  40 225]\n",
      " [141 299 317 299  88  53 299]\n",
      " [ 94 199 210 199  58  35 199]\n",
      " [122 260 276 260  77  46 260]\n",
      " [168 357 378 357 105  63 357]\n",
      " [136 289 306 289  85  51 289]\n",
      " [101 216 228 216  63  38 216]\n",
      " [168 357 378 357 105  63 357]\n",
      " [ 76 161 170 161  47  28 161]\n",
      " [ 82 174 185 174  51  31 174]\n",
      " [ 88 187 198 187  55  33 187]\n",
      " [ 84 179 189 179  53  32 179]]\n",
      "x= 3 alpha= 1 beta= 2\n",
      "[[ 53 239 268 239  21   7 239]\n",
      " [ 82 372 417 372  32  12 372]\n",
      " [ 48 217 243 217  19   7 217]\n",
      " [ 43 195 219 195  17   6 195]\n",
      " [ 88 398 446 398  34  12 398]\n",
      " [ 48 216 242 216  19   7 216]\n",
      " [ 68 309 347 309  27  10 309]\n",
      " [ 44 200 224 200  17   6 200]\n",
      " [ 54 244 274 244  21   8 244]\n",
      " [ 89 400 449 400  35  12 400]\n",
      " [ 61 277 311 277  24   9 277]\n",
      " [ 47 212 238 212  18   7 212]\n",
      " [ 89 400 449 400  35  12 400]\n",
      " [ 40 178 200 178  15   6 178]\n",
      " [ 43 194 217 194  17   6 194]\n",
      " [ 46 207 233 207  18   6 207]\n",
      " [ 44 199 223 199  17   6 199]]\n",
      "x= 3 alpha= 1 beta= 6\n",
      "[[  2 167 236 167   0   0 167]\n",
      " [  4 348 491 348   0   0 348]\n",
      " [  2 203 286 203   0   0 203]\n",
      " [  2 176 248 176   0   0 176]\n",
      " [  4 402 566 402   0   0 402]\n",
      " [  2 206 290 206   0   0 206]\n",
      " [  3 243 342 243   0   0 243]\n",
      " [  2 197 278 197   0   0 197]\n",
      " [  2 210 296 210   0   0 210]\n",
      " [  4 404 569 404   0   0 404]\n",
      " [  2 218 308 218   0   0 218]\n",
      " [  2 207 291 207   0   0 207]\n",
      " [  4 404 569 404   0   0 404]\n",
      " [  2 180 254 180   0   0 180]\n",
      " [  2 195 275 195   0   0 195]\n",
      " [  2 209 295 209   0   0 209]\n",
      " [  2 200 282 200   0   0 200]]\n",
      "x= 3 alpha= 2 beta= 1\n",
      "[[175 373 395 373 110  66 373]\n",
      " [237 504 534 504 148  89 504]\n",
      " [131 279 296 279  82  49 279]\n",
      " [127 269 285 269  79  48 269]\n",
      " [248 528 559 528 155  93 528]\n",
      " [126 268 284 268  79  47 268]\n",
      " [209 444 470 444 131  78 444]\n",
      " [105 223 236 223  66  39 223]\n",
      " [158 336 356 336  99  59 336]\n",
      " [249 530 561 530 156  93 530]\n",
      " [185 392 415 392 115  69 392]\n",
      " [117 248 263 248  73  44 248]\n",
      " [249 530 561 530 156  94 530]\n",
      " [ 77 163 172 163  48  29 163]\n",
      " [ 83 176 186 176  52  31 176]\n",
      " [ 89 188 200 188  55  33 188]\n",
      " [ 85 181 191 181  53  32 181]]\n",
      "x= 3 alpha= 2 beta= 2\n",
      "[[ 75 340 381 340  29  11 340]\n",
      " [122 550 617 550  48  17 550]\n",
      " [ 52 233 262 233  20   7 233]\n",
      " [ 48 216 243 216  19   7 216]\n",
      " [131 592 664 592  51  18 592]\n",
      " [ 51 228 256 228  20   7 228]\n",
      " [ 99 446 500 446  39  14 446]\n",
      " [ 45 203 228 203  18   6 203]\n",
      " [ 62 281 316 281  24   9 281]\n",
      " [132 594 666 594  51  18 594]\n",
      " [ 77 347 389 347  30  11 347]\n",
      " [ 48 219 245 219  19   7 219]\n",
      " [132 594 666 594  51  19 594]\n",
      " [ 40 179 200 179  15   6 179]\n",
      " [ 43 194 217 194  17   6 194]\n",
      " [ 46 207 233 207  18   6 207]\n",
      " [ 44 199 223 199  17   6 199]]\n",
      "x= 3 alpha= 2 beta= 6\n",
      "[[  2 195 274 195   0   0 195]\n",
      " [  5 501 706 501   0   0 501]\n",
      " [  2 203 286 203   0   0 203]\n",
      " [  2 176 248 176   0   0 176]\n",
      " [  6 597 841 597   0   0 597]\n",
      " [  2 206 290 206   0   0 206]\n",
      " [  3 311 439 311   0   0 311]\n",
      " [  2 197 278 197   0   0 197]\n",
      " [  2 211 298 211   0   0 211]\n",
      " [  7 599 844 599   0   0 599]\n",
      " [  2 227 321 227   0   0 227]\n",
      " [  2 207 291 207   0   0 207]\n",
      " [  7 599 845 599   0   0 599]\n",
      " [  2 180 254 180   0   0 180]\n",
      " [  2 195 275 195   0   0 195]\n",
      " [  2 209 295 209   0   0 209]\n",
      " [  2 200 282 200   0   0 200]]\n",
      "x= 3 alpha= 3 beta= 1\n",
      "[[234 497 526 497 146  88 497]\n",
      " [315 670 710 670 197 118 670]\n",
      " [155 330 349 330  97  58 330]\n",
      " [154 326 346 326  96  58 326]\n",
      " [330 701 742 701 206 124 701]\n",
      " [147 312 330 312  92  55 312]\n",
      " [277 589 624 589 173 104 589]\n",
      " [116 247 262 247  73  44 247]\n",
      " [194 412 437 412 121  73 412]\n",
      " [331 703 744 703 207 124 703]\n",
      " [233 496 525 496 146  88 496]\n",
      " [132 281 298 281  83  50 281]\n",
      " [331 703 744 703 207 124 703]\n",
      " [ 77 164 174 164  48  29 164]\n",
      " [ 84 178 188 178  52  31 178]\n",
      " [ 89 190 201 190  56  34 190]\n",
      " [ 86 182 193 182  54  32 182]]\n",
      "x= 3 alpha= 3 beta= 2\n",
      "[[ 97 440 494 440  38  14 440]\n",
      " [161 729 817 729  63  23 729]\n",
      " [ 55 250 280 250  22   8 250]\n",
      " [ 53 237 266 237  21   7 237]\n",
      " [174 786 881 786  68  24 786]\n",
      " [ 53 240 269 240  21   7 240]\n",
      " [129 583 653 583  50  18 583]\n",
      " [ 46 207 232 207  18   6 207]\n",
      " [ 71 319 358 319  28  10 319]\n",
      " [174 788 883 788  68  25 788]\n",
      " [ 92 417 467 417  36  13 417]\n",
      " [ 50 226 253 226  20   7 226]\n",
      " [174 788 883 788  68  25 788]\n",
      " [ 40 179 200 179  15   6 179]\n",
      " [ 43 194 217 194  17   6 194]\n",
      " [ 46 207 233 207  18   6 207]\n",
      " [ 44 199 223 199  17   6 199]]\n",
      "x= 3 alpha= 3 beta= 6\n",
      "[[   2  222  313  222    0    0  222]\n",
      " [   7  654  922  654    0    0  654]\n",
      " [   2  203  286  203    0    0  203]\n",
      " [   2  177  249  177    0    0  177]\n",
      " [   9  793 1117  793    1    0  793]\n",
      " [   2  206  290  206    0    0  206]\n",
      " [   4  380  536  380    0    0  380]\n",
      " [   2  197  278  197    0    0  197]\n",
      " [   2  213  300  213    0    0  213]\n",
      " [   9  795 1120  795    1    0  795]\n",
      " [   3  237  333  237    0    0  237]\n",
      " [   2  207  291  207    0    0  207]\n",
      " [   9  795 1120  795    1    0  795]\n",
      " [   2  180  254  180    0    0  180]\n",
      " [   2  195  275  195    0    0  195]\n",
      " [   2  209  295  209    0    0  209]\n",
      " [   2  200  282  200    0    0  200]]\n",
      "x= 4 alpha= 1 beta= 1\n",
      "[[166 352 373 352 103  62 352]\n",
      " [214 456 483 456 134  80 456]\n",
      " [145 308 326 308  91  54 308]\n",
      " [138 293 311 293  86  52 293]\n",
      " [223 475 503 475 140  84 475]\n",
      " [142 302 319 302  89  53 302]\n",
      " [193 410 434 410 120  72 410]\n",
      " [127 269 285 269  79  47 269]\n",
      " [164 348 368 348 102  61 348]\n",
      " [224 477 505 477 140  84 477]\n",
      " [181 385 408 385 113  68 385]\n",
      " [136 289 306 289  85  51 289]\n",
      " [224 477 505 477 140  84 477]\n",
      " [105 224 237 224  66  39 224]\n",
      " [111 237 251 237  70  42 237]\n",
      " [117 249 264 249  73  44 249]\n",
      " [114 242 256 242  71  43 242]]\n",
      "x= 4 alpha= 1 beta= 2\n",
      "[[ 76 342 384 342  30  11 342]\n",
      " [111 501 562 501  43  16 501]\n",
      " [ 65 292 327 292  25   9 292]\n",
      " [ 60 272 305 272  24   8 272]\n",
      " [118 532 597 532  46  17 532]\n",
      " [ 64 290 325 290  25   9 290]\n",
      " [ 94 424 476 424  37  13 424]\n",
      " [ 60 270 303 270  23   8 270]\n",
      " [ 72 326 365 326  28  10 326]\n",
      " [118 534 599 534  46  17 534]\n",
      " [ 82 370 415 370  32  12 370]\n",
      " [ 63 284 318 284  25   9 284]\n",
      " [118 534 599 534  46  17 534]\n",
      " [ 55 248 278 248  21   8 248]\n",
      " [ 58 263 295 263  23   8 263]\n",
      " [ 61 277 310 277  24   9 277]\n",
      " [ 59 268 301 268  23   8 268]]\n",
      "x= 4 alpha= 1 beta= 6\n",
      "[[  3 247 348 247   0   0 247]\n",
      " [  5 469 661 469   0   0 469]\n",
      " [  3 273 385 273   0   0 273]\n",
      " [  3 246 347 246   0   0 246]\n",
      " [  6 537 757 537   0   0 537]\n",
      " [  3 276 389 276   0   0 276]\n",
      " [  4 336 473 336   0   0 336]\n",
      " [  3 268 377 268   0   0 268]\n",
      " [  3 280 395 280   0   0 280]\n",
      " [  6 539 760 539   0   0 539]\n",
      " [  3 292 411 292   0   0 292]\n",
      " [  3 277 390 277   0   0 277]\n",
      " [  6 539 760 539   0   0 539]\n",
      " [  3 250 353 250   0   0 250]\n",
      " [  3 265 374 265   0   0 265]\n",
      " [  3 279 394 279   0   0 279]\n",
      " [  3 271 381 271   0   0 271]]\n",
      "x= 4 alpha= 2 beta= 1\n",
      "[[244 518 548 518 152  91 518]\n",
      " [319 677 717 677 199 119 677]\n",
      " [176 375 397 375 110  66 375]\n",
      " [174 369 391 369 109  65 369]\n",
      " [332 705 747 705 207 124 705]\n",
      " [169 359 380 359 106  63 359]\n",
      " [284 603 638 603 177 106 603]\n",
      " [142 301 319 301  89  53 301]\n",
      " [211 449 475 449 132  79 449]\n",
      " [333 707 749 707 208 125 707]\n",
      " [246 523 554 523 154  92 523]\n",
      " [156 332 352 332  98  59 332]\n",
      " [333 707 749 707 208 125 707]\n",
      " [106 226 239 226  66  40 226]\n",
      " [113 239 253 239  70  42 239]\n",
      " [118 252 266 252  74  44 252]\n",
      " [115 244 258 244  72  43 244]]\n",
      "x= 4 alpha= 2 beta= 2\n",
      "[[105 476 534 476  41  15 476]\n",
      " [164 739 828 739  64  23 739]\n",
      " [ 69 314 352 314  27  10 314]\n",
      " [ 66 300 336 300  26   9 300]\n",
      " [175 790 886 790  68  25 790]\n",
      " [ 68 306 343 306  26  10 306]\n",
      " [134 607 680 607  52  19 607]\n",
      " [ 61 275 309 275  24   9 275]\n",
      " [ 83 376 422 376  33  12 376]\n",
      " [175 792 888 792  69  25 792]\n",
      " [102 463 519 463  40  14 463]\n",
      " [ 65 293 329 293  25   9 293]\n",
      " [176 793 889 793  69  25 793]\n",
      " [ 55 248 278 248  21   8 248]\n",
      " [ 58 263 295 263  23   8 263]\n",
      " [ 61 277 310 277  24   9 277]\n",
      " [ 59 268 301 268  23   8 268]]\n",
      "x= 4 alpha= 2 beta= 6\n",
      "[[   3  283  399  283    0    0  283]\n",
      " [   7  673  949  673    0    0  673]\n",
      " [   3  273  385  273    0    0  273]\n",
      " [   3  247  347  247    0    0  247]\n",
      " [   9  798 1124  798    1    0  798]\n",
      " [   3  276  389  276    0    0  276]\n",
      " [   5  427  602  427    0    0  427]\n",
      " [   3  268  377  268    0    0  268]\n",
      " [   3  282  398  282    0    0  282]\n",
      " [   9  800 1127  800    1    0  800]\n",
      " [   3  304  428  304    0    0  304]\n",
      " [   3  277  390  277    0    0  277]\n",
      " [   9  800 1127  800    1    0  800]\n",
      " [   3  250  353  250    0    0  250]\n",
      " [   3  265  374  265    0    0  265]\n",
      " [   3  279  394  279    0    0  279]\n",
      " [   3  271  381  271    0    0  271]]\n",
      "x= 4 alpha= 3 beta= 1\n",
      "[[322 684 724 684 201 121 684]\n",
      " [423 898 951 898 264 158 898]\n",
      " [208 442 468 442 130  78 442]\n",
      " [210 445 472 445 131  79 445]\n",
      " [440 936 991 936 275 165 936]\n",
      " [196 417 441 417 123  74 417]\n",
      " [375 797 843 797 234 141 797]\n",
      " [157 333 353 333  98  59 333]\n",
      " [259 550 583 550 162  97 550]\n",
      " [441 937 992 937 276 165 937]\n",
      " [311 662 701 662 195 117 662]\n",
      " [177 376 398 376 111  66 376]\n",
      " [441 938 993 938 276 165 938]\n",
      " [107 228 242 228  67  40 228]\n",
      " [114 242 256 242  71  43 242]\n",
      " [119 254 269 254  75  45 254]\n",
      " [116 246 261 246  72  43 246]]\n",
      "x= 4 alpha= 3 beta= 2\n",
      "[[ 135  610  684  610   53   19  610]\n",
      " [ 216  977 1095  977   85   30  977]\n",
      " [  74  336  376  336   29   10  336]\n",
      " [  73  328  368  328   28   10  328]\n",
      " [ 232 1049 1176 1049   91   33 1049]\n",
      " [  71  322  361  322   28   10  322]\n",
      " [ 175  789  884  789   68   25  789]\n",
      " [  62  280  314  280   24    9  280]\n",
      " [  94  426  478  426   37   13  426]\n",
      " [ 233 1051 1178 1051   91   33 1051]\n",
      " [ 123  556  623  556   48   17  556]\n",
      " [  67  302  339  302   26    9  302]\n",
      " [ 233 1051 1178 1051   91   33 1051]\n",
      " [  55  248  278  248   21    8  248]\n",
      " [  58  263  295  263   23    8  263]\n",
      " [  61  277  311  277   24    9  277]\n",
      " [  59  268  301  268   23    8  268]]\n",
      "x= 4 alpha= 3 beta= 6\n",
      "[[   3  319  450  319    0    0  319]\n",
      " [  10  877 1236  877    1    0  877]\n",
      " [   3  273  385  273    0    0  273]\n",
      " [   3  247  348  247    0    0  247]\n",
      " [  11 1058 1491 1058    1    0 1058]\n",
      " [   3  276  389  276    0    0  276]\n",
      " [   6  519  731  519    0    0  519]\n",
      " [   3  268  377  268    0    0  268]\n",
      " [   3  284  400  284    0    0  284]\n",
      " [  12 1060 1494 1060    1    0 1060]\n",
      " [   3  316  445  316    0    0  316]\n",
      " [   3  277  390  277    0    0  277]\n",
      " [  12 1060 1494 1060    1    0 1060]\n",
      " [   3  250  353  250    0    0  250]\n",
      " [   3  265  374  265    0    0  265]\n",
      " [   3  279  394  279    0    0  279]\n",
      " [   3  271  381  271    0    0  271]]\n",
      "x= 5 alpha= 1 beta= 1\n",
      "[[214 455 482 455 134  80 455]\n",
      " [270 573 607 573 169 101 573]\n",
      " [182 387 409 387 114  68 387]\n",
      " [176 374 396 374 110  66 374]\n",
      " [280 594 629 594 175 105 594]\n",
      " [178 378 400 378 111  67 378]\n",
      " [245 520 550 520 153  92 520]\n",
      " [159 339 359 339 100  60 339]\n",
      " [205 435 460 435 128  77 435]\n",
      " [281 596 631 596 175 105 596]\n",
      " [227 482 510 482 142  85 482]\n",
      " [170 362 383 362 106  64 362]\n",
      " [281 596 631 596 175 105 596]\n",
      " [135 286 303 286  84  50 286]\n",
      " [141 299 317 299  88  53 299]\n",
      " [147 312 330 312  92  55 312]\n",
      " [143 304 322 304  89  54 304]]\n",
      "x= 5 alpha= 1 beta= 2\n",
      "[[ 99 445 499 445  39  14 445]\n",
      " [140 630 706 630  54  20 630]\n",
      " [ 81 367 411 367  32  11 367]\n",
      " [ 77 348 391 348  30  11 348]\n",
      " [148 666 747 666  58  21 666]\n",
      " [ 80 363 407 363  31  11 363]\n",
      " [119 539 605 539  47  17 539]\n",
      " [ 76 341 382 341  30  11 341]\n",
      " [ 90 408 457 408  35  13 408]\n",
      " [148 668 749 668  58  21 668]\n",
      " [102 463 519 463  40  14 463]\n",
      " [ 79 356 399 356  31  11 356]\n",
      " [148 669 749 669  58  21 669]\n",
      " [ 70 318 356 318  27  10 318]\n",
      " [ 74 333 373 333  29  10 333]\n",
      " [ 77 346 388 346  30  11 346]\n",
      " [ 75 338 379 338  29  11 338]]\n",
      "x= 5 alpha= 1 beta= 6\n",
      "[[  4 326 459 326   0   0 326]\n",
      " [  6 590 832 590   0   0 590]\n",
      " [  4 343 484 343   0   0 343]\n",
      " [  3 316 446 316   0   0 316]\n",
      " [  7 672 947 672   0   0 672]\n",
      " [  4 346 488 346   0   0 346]\n",
      " [  5 429 604 429   0   0 429]\n",
      " [  4 338 476 338   0   0 338]\n",
      " [  4 351 495 351   0   0 351]\n",
      " [  7 674 950 674   0   0 674]\n",
      " [  4 365 514 365   0   0 365]\n",
      " [  4 347 489 347   0   0 347]\n",
      " [  7 675 950 675   0   0 675]\n",
      " [  3 320 451 320   0   0 320]\n",
      " [  4 335 473 335   0   0 335]\n",
      " [  4 349 492 349   0   0 349]\n",
      " [  4 341 480 341   0   0 341]]\n",
      "x= 5 alpha= 2 beta= 1\n",
      "[[312 663 702 663 195 117 663]\n",
      " [400 849 899 849 250 150 849]\n",
      " [221 470 498 470 138  83 470]\n",
      " [221 469 497 469 138  83 469]\n",
      " [415 882 934 882 260 156 882]\n",
      " [212 450 476 450 132  79 450]\n",
      " [358 762 807 762 224 134 762]\n",
      " [178 379 402 379 112  67 379]\n",
      " [264 562 595 562 165  99 562]\n",
      " [416 884 936 884 260 156 884]\n",
      " [308 655 693 655 193 116 655]\n",
      " [196 416 441 416 122  73 416]\n",
      " [416 884 936 884 260 156 884]\n",
      " [136 289 306 289  85  51 289]\n",
      " [142 302 320 302  89  53 302]\n",
      " [148 315 333 315  93  56 315]\n",
      " [144 307 325 307  90  54 307]]\n",
      "x= 5 alpha= 2 beta= 2\n",
      "[[ 136  613  687  613   53   19  613]\n",
      " [ 205  927 1040  927   80   29  927]\n",
      " [  87  394  442  394   34   12  394]\n",
      " [  85  384  430  384   33   12  384]\n",
      " [ 219  989 1109  989   86   31  989]\n",
      " [  85  383  430  383   33   12  383]\n",
      " [ 170  767  860  767   66   24  767]\n",
      " [  77  347  389  347   30   11  347]\n",
      " [ 104  470  527  470   41   15  470]\n",
      " [ 219  991 1111  991   86   31  991]\n",
      " [ 128  579  649  579   50   18  579]\n",
      " [  81  367  412  367   32   11  367]\n",
      " [ 220  991 1111  991   86   31  991]\n",
      " [  70  318  356  318   27   10  318]\n",
      " [  74  333  373  333   29   10  333]\n",
      " [  77  346  388  346   30   11  346]\n",
      " [  75  338  379  338   29   11  338]]\n",
      "x= 5 alpha= 2 beta= 6\n",
      "[[   4  371  523  371    0    0  371]\n",
      " [   9  845 1191  845    1    0  845]\n",
      " [   4  343  484  343    0    0  343]\n",
      " [   3  317  446  317    0    0  317]\n",
      " [  11  998 1406  998    1    0  998]\n",
      " [   4  346  488  346    0    0  346]\n",
      " [   6  543  766  543    0    0  543]\n",
      " [   4  338  476  338    0    0  338]\n",
      " [   4  353  498  353    0    0  353]\n",
      " [  11 1000 1409 1000    1    0 1000]\n",
      " [   4  380  535  380    0    0  380]\n",
      " [   4  347  489  347    0    0  347]\n",
      " [  11 1000 1409 1000    1    0 1000]\n",
      " [   3  320  451  320    0    0  320]\n",
      " [   4  335  473  335    0    0  335]\n",
      " [   4  349  492  349    0    0  349]\n",
      " [   4  341  480  341    0    0  341]]\n",
      "x= 5 alpha= 3 beta= 1\n",
      "[[ 409  870  921  870  256  154  870]\n",
      " [ 530 1126 1192 1126  331  199 1126]\n",
      " [ 261  554  586  554  163   98  554]\n",
      " [ 266  564  598  564  166  100  564]\n",
      " [ 551 1170 1239 1170  344  207 1170]\n",
      " [ 246  522  553  522  154   92  522]\n",
      " [ 472 1004 1063 1004  295  177 1004]\n",
      " [ 197  420  444  420  123   74  420]\n",
      " [ 324  688  729  688  202  121  688]\n",
      " [ 552 1172 1241 1172  345  207 1172]\n",
      " [ 389  827  876  827  243  146  827]\n",
      " [ 222  471  499  471  139   83  471]\n",
      " [ 552 1172 1241 1172  345  207 1172]\n",
      " [ 137  292  309  292   86   52  292]\n",
      " [ 144  305  323  305   90   54  305]\n",
      " [ 149  318  336  318   93   56  318]\n",
      " [ 146  310  328  310   91   55  310]]\n",
      "x= 5 alpha= 3 beta= 2\n",
      "[[ 173  780  875  780   67   24  780]\n",
      " [ 271 1225 1373 1225  106   38 1225]\n",
      " [  93  421  472  421   36   13  421]\n",
      " [  93  419  469  419   36   13  419]\n",
      " [ 291 1312 1471 1312  113   41 1312]\n",
      " [  89  404  452  404   35   13  404]\n",
      " [ 220  995 1115  995   86   31  995]\n",
      " [  78  354  397  354   31   11  354]\n",
      " [ 118  533  598  533   46   17  533]\n",
      " [ 291 1314 1473 1314  114   41 1314]\n",
      " [ 154  695  779  695   60   22  695]\n",
      " [  84  379  425  379   33   12  379]\n",
      " [ 291 1314 1473 1314  114   41 1314]\n",
      " [  70  318  356  318   27   10  318]\n",
      " [  74  333  373  333   29   10  333]\n",
      " [  77  346  388  346   30   11  346]\n",
      " [  75  338  379  338   29   11  338]]\n",
      "x= 5 alpha= 3 beta= 6\n",
      "[[   5  417  587  417    0    0  417]\n",
      " [  12 1100 1550 1100    1    0 1100]\n",
      " [   4  344  484  344    0    0  344]\n",
      " [   3  317  447  317    0    0  317]\n",
      " [  14 1324 1865 1324    1    0 1324]\n",
      " [   4  346  488  346    0    0  346]\n",
      " [   7  658  927  658    0    0  658]\n",
      " [   4  338  476  338    0    0  338]\n",
      " [   4  356  501  356    0    0  356]\n",
      " [  14 1326 1868 1326    1    0 1326]\n",
      " [   4  395  557  395    0    0  395]\n",
      " [   4  347  489  347    0    0  347]\n",
      " [  14 1326 1868 1326    1    0 1326]\n",
      " [   3  320  451  320    0    0  320]\n",
      " [   4  335  473  335    0    0  335]\n",
      " [   4  349  492  349    0    0  349]\n",
      " [   4  341  480  341    0    0  341]]\n"
     ]
    }
   ],
   "source": [
    "mx_wndws=int(np.max(cls_wndws)+1)   # حداکثر تعداد پنجره ی موجود بین کلاس ها\n",
    "print('\\n max =', mx_wndws)\n",
    "rpt_cnn_clsfctn=3                 #تعداد اجرای شبکه عصبی برای میانگین گیری هر حالت \n",
    "aug_amnt_Ttl=np.empty((0,7), int) \n",
    "rslts=np.array([])\n",
    "for X in range(1,6):                               #داده افزایی تا چند برابر کلاس اکثریت\n",
    "    mx_wndws=int(np.max(cls_wndws))   # حداکثر تعداد پنجره ی موجود بین کلاس ها\n",
    "    mx_wndws=int(X*mx_wndws)          #\n",
    "    for alpha in range(1,4):\n",
    "        rprt=np.array([])\n",
    "        for beta in range(1,4):                              #چند برابر کردن داده افزایی هر کلاس متناسب ضعیف بودن آن کلاس\n",
    "            if beta==3:\n",
    "                beta=beta*2\n",
    "\n",
    "            f_scr=np.array([57,65,74,75,74,62,60,74])     #UnAg,Scl.2,Mag.05,Tm.2,Gs1,frqnc2,GAN,LSTMexpgdo\n",
    "            #f_scr=np.array([51,68,73,74,71,56,55,70])      #UnAg,Scl.2,Mag.05,Tm.2,Gs1,frqnc2,GAN,LSTMexpgdo\n",
    "            f_scr_difrnc=f_scr-f_scr[0]\n",
    "            for z in range (len(f_scr_difrnc)):\n",
    "                if f_scr_difrnc[z]<0:\n",
    "                    f_scr_difrnc[z]=0\n",
    "            f_scr_difrnc=f_scr_difrnc**beta\n",
    "            f_scr_rtio=f_scr_difrnc/(sum(f_scr_difrnc))\n",
    "            aug_amnt=np.empty((0,len(f_scr_rtio)-1), int) \n",
    "\n",
    "            scors0=np.array([28,4,71,67,0,75,16,86,56,0,40,81,0,99,99,99,99])         #Class Recalls without Augmentation\n",
    "            #scors0=np.array([[28,4,71,67,0,75,16,86,56,0,40,81,0,99,100,100,100])     #Class Recalls without Augmentation\n",
    "            MxMn20fscr=((max(f_scr)-f_scr[0])/20)**0.7\n",
    "            AgScr=1+alpha*MxMn20fscr*((100-scors0)/(np.max(100-scors0)))**beta        #Class Specific\n",
    "            \n",
    "            for i in range (1,cls_num+1):\n",
    "                aug_amnt=np.append(aug_amnt,[f_scr_rtio[1:]*(mx_wndws*AgScr[i-1]-cls_wndws[i-1])],axis=0)\n",
    "                #vlum_win=mx_wndws-cls_wndws[i-1]                        #میزان داده افزایی در روش های عمیق (پنجره)\n",
    "            aug_amnt=np.int16(np.round(aug_amnt))\n",
    "            aug_amnt_Ttl=np.append(aug_amnt_Ttl,aug_amnt,axis=0)\n",
    "            #print(np.int16(100*f_scr_rtio))\n",
    "            #print(aug_amnt)\n",
    "            rprt=np.empty((0,3),float)\n",
    "            print('x=',X,'alpha=',alpha,'beta=',beta)\n",
    "            print(aug_amnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " max = 2599\n",
      "[[ 164  348  368  348  102   61  348]\n",
      " [ 388  825  874  825  243  146  825]\n",
      " [ 289  615  651  615  181  108  615]\n",
      " [ 172  366  388  366  108   65  366]\n",
      " [ 457  971 1028  971  285  171  971]\n",
      " [ 284  602  638  602  177  106  602]\n",
      " [ 306  651  689  651  191  115  651]\n",
      " [ 224  477  505  477  140   84  477]\n",
      " [ 334  709  751  709  209  125  709]\n",
      " [ 462  981 1039  981  289  173  981]\n",
      " [ 374  795  842  795  234  140  795]\n",
      " [ 278  590  625  590  174  104  590]\n",
      " [ 457  971 1028  971  286  171  971]\n",
      " [ 151  322  341  322   95   57  322]\n",
      " [ 187  398  421  398  117   70  398]\n",
      " [ 243  515  546  515  152   91  515]\n",
      " [ 204  434  460  434  128   77  434]]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/sequential/batch_normalization/moments/BroadcastTo_1' defined at (most recent call last):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\IP330\\AppData\\Local\\Temp/ipykernel_10936/377892515.py\", line 189, in <module>\n      model_crs.fit(X_train, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_data = (X_valid, y_valid_cat), callbacks =[earlystopping])\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 530, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 583, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 464, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/sequential/batch_normalization/moments/BroadcastTo_1'\nOOM when allocating tensor with shape[1000,469,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/sequential/batch_normalization/moments/BroadcastTo_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2335]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10936/377892515.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mearlystopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"val_accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"max\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                 \u001b[0mmodel_crs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid_cat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlystopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[1;31m#_, accuracy = model_crs.evaluate(X_valid, y_valid_cat, batch_size=batch_size, verbose=verbose)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/sequential/batch_normalization/moments/BroadcastTo_1' defined at (most recent call last):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n      await self.process_one()\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n      await dispatch(*args)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n      await result\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n      reply_content = await reply_content\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2901, in run_cell\n      result = self._run_cell(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n      return runner(coro)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3172, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\IP330\\AppData\\Local\\Temp/ipykernel_10936/377892515.py\", line 189, in <module>\n      model_crs.fit(X_train, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_data = (X_valid, y_valid_cat), callbacks =[earlystopping])\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 530, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 583, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py\", line 464, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/sequential/batch_normalization/moments/BroadcastTo_1'\nOOM when allocating tensor with shape[1000,469,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node gradient_tape/sequential/batch_normalization/moments/BroadcastTo_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_2335]"
     ]
    }
   ],
   "source": [
    "            for repeat in range(1,rpt_cnn_clsfctn+1):\n",
    "                xtrain=np.empty((0,smpl_rte+1), float) \n",
    "                for i in range (1,cls_num+1):                            #تعریف آرایه ی پنجره های کلاس ها\n",
    "                    xtrain=np.append(xtrain,Aug_data(cwd,i,'00',cls_wndws[i-1]),axis=0)\n",
    "                    if aug_amnt[i-1,0]>0:\n",
    "                        methd=10\n",
    "                        sort=-3  #-5=Dis_sam_Cls=qlty1 #-4=var_Dis=qlty2 #-3=DisSm/DisOthr=qlty3  #-2=invrs_GDO_papr=qlty4   #-1=GDO_papr=qlty5\n",
    "                        dp_slct_dstrb=4            #1-sequencial 2-linear 3-beta 4-exponential 5-unfrm\n",
    "                        xtrain=np.append(xtrain,Aug_data(cwd,i,10,aug_amnt[i-1,0]),axis=0)\n",
    "                    if aug_amnt[i-1,1]>0:\n",
    "                        methd=20\n",
    "                        sort=-3\n",
    "                        dp_slct_dstrb=2\n",
    "                        xtrain=np.append(xtrain,Aug_data(cwd,i,20,aug_amnt[i-1,1]),axis=0)\n",
    "                    if aug_amnt[i-1,2]>0:\n",
    "                        methd=30\n",
    "                        sort=-3\n",
    "                        dp_slct_dstrb=4\n",
    "                        xtrain=np.append(xtrain,Aug_data(cwd,i,30,aug_amnt[i-1,2]),axis=0)\n",
    "                    if aug_amnt[i-1,3]>0:\n",
    "                        methd=40\n",
    "                        sort=-1\n",
    "                        dp_slct_dstrb=4\n",
    "                        xtrain=np.append(xtrain,Aug_data(cwd,i,40,aug_amnt[i-1,3]),axis=0)\n",
    "                    if aug_amnt[i-1,4]>0:\n",
    "                        methd=50\n",
    "                        sort=-3\n",
    "                        dp_slct_dstrb=4\n",
    "                        xtrain=np.append(xtrain,Aug_data(cwd,i,50,aug_amnt[i-1,4]),axis=0)\n",
    "                    if aug_amnt[i-1,5]>0:\n",
    "                        methd=60\n",
    "                        sort=-3\n",
    "                        dp_slct_dstrb=4                             \n",
    "                        xtrain=np.append(xtrain,Aug_data(cwd,i,60,aug_amnt[i-1,5]),axis=0) #Scale\n",
    "                    if aug_amnt[i-1,6]>0:\n",
    "                        methd=70\n",
    "                        sort=-3\n",
    "                        dp_slct_dstrb=4\n",
    "                        xtrain=np.append(xtrain,Aug_data(cwd,i,70,aug_amnt[i-1,6]),axis=0)\n",
    "                             \n",
    "                mx_aug=np.max(xtrain[:,:-1])\n",
    "                mx=np.max(ecg[:,:-1])\n",
    "                mn_aug=np.min(xtrain[:,:-1])\n",
    "                mn=np.min(ecg[:,:-1])\n",
    "\n",
    "                #for i in range (1,18):             #نرمالسازی داده های افزایشی\n",
    "                xtrain[:,:-1]= 2*(xtrain[:,:-1]-mn_aug)/(mx_aug - mn_aug) - 1\n",
    "\n",
    "                wndws_test=np.empty((0,smpl_rte+1), float)\n",
    "                smpl_rte=480                                              # در ماژول ها نیز همین مقدار ثبت شده\n",
    "                i=0\n",
    "                windws=np.array([])\n",
    "                for cls in range (1,18):                                 #ساخت پنجره های داده های آموزش اصلی\n",
    "                    dta=np.array(vars()['ecg'+str(cls)][:int(.1*len(vars()['ecg'+str(cls)])),:]) #انتخاب محدوده ترِین جهت داده افزایی\n",
    "                    vars()['wndws_tst'+str(cls)]=wndwng(dta)                #نرخ نمونه برداری 480 است و شماره کلاس در آخر ردیف نیست\n",
    "                    clm0vlu=np.zeros(len(vars()['wndws_tst'+str(cls)]))\n",
    "                    cls_clm=np.array(clm0vlu)\n",
    "                    cls_clm=np.int16(cls_clm+cls)\n",
    "                    cls_clm=np.transpose([cls_clm])                     # افزودن ستون برچسب (شماره کلاس) به سمت راست پنجره ها\n",
    "                    vars()['wndws_tst'+str(cls)]=np.concatenate((vars()['wndws_tst'+str(cls)],cls_clm),axis=1)   \n",
    "\n",
    "                #print(np.max(wndws_tst))\n",
    "                for i in range (1,18):                # Normalization test windows\n",
    "                    cls=i\n",
    "                    '''print('\\n cls', i, ' >> ')\n",
    "                    print('max magnitude class', i , ' = ' ,np.max(vars()['wndws_tst'+str(cls)][:,:-1]))\n",
    "                    print('min magnitude class', i , ' = ' ,np.min(vars()['wndws_tst'+str(cls)][:,:-1]))'''\n",
    "                    vars()['wndws_tst'+str(i)][:,:-1]= 2*(vars()['wndws_tst'+str(i)][:,:-1]-mn_aug)/(mx_aug - mn_aug) - 1\n",
    "                    '''print('after normalizing >>')\n",
    "                    print('max magnitude class', i , ' = ' ,np.max(vars()['wndws_tst'+str(cls)][:,:-1]))\n",
    "                    print('min magnitude class', i , ' = ' ,np.min(vars()['wndws_tst'+str(cls)][:,:-1]))'''\n",
    "\n",
    "                #print(\"each class and its windows =\")                    #تجمیع کلاس های تست\n",
    "                cls_wndws=np.array([])\n",
    "                for cls in range (1,18):                                # آرایه ی تعداد پنجره ی هر کلاس\n",
    "                    wns=len(vars()['wndws_tst'+str(cls)])\n",
    "                    cls_wndws=np.append(cls_wndws,wns)\n",
    "                    #print(cls, wns, end='\\t')\n",
    "                    wndws_test=np.append(wndws_test,vars()['wndws_tst'+str(cls)],axis=0)\n",
    "\n",
    "                #mx_wndws=int(np.max(cls_wndws))                        # حداکثر تعداد پنجره ی موجود بین کلاس ها\n",
    "                #print('\\n max instance in classes of test windows =', mx_wndws)\n",
    "\n",
    "                Xtest=wndws_test[:,:-1]\n",
    "                ytest=np.int16(wndws_test[:,-1])\n",
    "\n",
    "                trainx=np.random.permutation(xtrain)\n",
    "                Xtrain=np.array(trainx[:,:-1])\n",
    "                ytrain=np.int16(trainx[:,-1])\n",
    "\n",
    "                X_train=np.array(Xtrain)\n",
    "                y_train=np.array(ytrain)\n",
    "                X_test=np.array(Xtest)\n",
    "                y_test=np.array(ytest)\n",
    "                X_valid=np.array(Xtrain)\n",
    "                y_valid=np.array(ytrain)\n",
    "\n",
    "                y_train_cat=to_categorical(y_train)\n",
    "                y_valid_cat=to_categorical(y_valid)\n",
    "                y_test_cat=to_categorical(y_test)\n",
    "                X_train = np.expand_dims(X_train, axis=2)\n",
    "                X_valid = np.expand_dims(X_valid, axis=2)\n",
    "                X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "                #VGG16_1D\n",
    "\n",
    "                accuracy=0\n",
    "                acc_crs=np.array([])\n",
    "                ##for i in range (10):\n",
    "                ##    if accuracy<0.2 :\n",
    "\n",
    "                verbose, epochs, batch_size = 0, 3, btch\n",
    "                n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train_cat.shape[1]\n",
    "                steps_per_epoch = len(X_train)//batch_size\n",
    "                validation_steps = len(X_valid)//batch_size # if you have test data\n",
    "\n",
    "\n",
    "\n",
    "                model_crs = Sequential()\n",
    "                #model_crs.add(Conv1D(input_shape=x_train.shape[1:],filters=64,kernel_size=9,padding=\"same\", activation=\"relu\"))\n",
    "                model_crs.add(Conv1D(filters=128, kernel_size=12, strides=1, activation='relu', input_shape=(n_timesteps,n_features))) #Replaced\n",
    "                model_crs.add(BatchNormalization())\n",
    "                model_crs.add(MaxPooling1D(pool_size=2,strides=3))\n",
    "\n",
    "                model_crs.add(Conv1D(filters=32, kernel_size=7, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "                model_crs.add(BatchNormalization())\n",
    "                model_crs.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "\n",
    "                model_crs.add(Conv1D(filters=32, kernel_size=10, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "                model_crs.add(Conv1D(filters=128, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\"))\n",
    "                model_crs.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "\n",
    "                model_crs.add(Conv1D(filters=256, kernel_size=15, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "                model_crs.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "\n",
    "                model_crs.add(Conv1D(filters=512, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "                #model_crs.add(Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "\n",
    "                model_crs.add(Flatten())\n",
    "                model_crs.add(Dropout(0.1))\n",
    "                model_crs.add(Dense(units=512,activation=\"relu\"))\n",
    "                #model_crs.add(Dense(units=2048,activation=\"relu\"))\n",
    "                model_crs.add(Dense(n_outputs, activation='softmax'))   #Replaced here from old Model\n",
    "\n",
    "                model_crs.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                # fit network   #CategoricalCrossentropy #sparse_categorical_crossentropy #SparseCategoricalCrossentropy\n",
    "\n",
    "                earlystopping = callbacks.EarlyStopping(monitor =\"val_accuracy\", mode =\"max\", patience = 50, restore_best_weights = True)\n",
    "\n",
    "                model_crs.fit(X_train, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_data = (X_valid, y_valid_cat), callbacks =[earlystopping])\n",
    "\n",
    "                #_, accuracy = model_crs.evaluate(X_valid, y_valid_cat, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "                #print('np.shape(X_test)=',np.shape(X_test))\n",
    "\n",
    "                #print('Accuracy= ', accuracy)\n",
    "\n",
    "                #y_pred_vgg = model_crs.predict_classes(X_test)\n",
    "                predict_x=model_crs.predict(X_test)              # Function 1\n",
    "\n",
    "                y_pred_crs=maxindx(predict_x)                    # function from augment.py to remove 0 index predictions\n",
    "\n",
    "                #y_pred_crs = model_crs.predict_classes(X_test)\n",
    "\n",
    "                cm = confusion_matrix(y_test, y_pred_crs)\n",
    "                #print(cm)\n",
    "                acc3=accuracy_score(y_test, y_pred_crs)\n",
    "\n",
    "                #print(classification_report(y_test, y_pred_crs))\n",
    "\n",
    "                #sns.heatmap(cm, annot=True)\n",
    "\n",
    "                #plt.imshow(cm)\n",
    "\n",
    "                ### Maxcount Predicting Test data\n",
    "                ecg_test=np.array(ecg1[:int(.1*len(ecg1)),:])\n",
    "                #print(1,len(ecg_test), end='\\t')\n",
    "                for i in range (2,18):\n",
    "                    ecg_test=np.concatenate((ecg_test,vars()['ecg'+str(i)][:int(.1*len(vars()['ecg'+str(i)])),:]),axis=0)\n",
    "                    #print(i,len(vars()['ecg'+str(i)]), end='\\t')    \n",
    "                #print('\\n ecg_test shape = ',np.shape(ecg_test))\n",
    "                i=0\n",
    "                mlpinputshps=np.array([])\n",
    "                maxcntlbl=np.array([])\n",
    "                mlpinput=np.array([])\n",
    "                mlplbl=np.array([])\n",
    "                cnnlbl=np.array([])\n",
    "                peaklens=np.array([])            #array of peaks quantity in each raw signal\n",
    "                x=np.array([])\n",
    "                wow=0\n",
    "\n",
    "                for i in range (len(ecg_test)): #calculating max peaks in row signals\n",
    "                    peaklens=np.append(peaklens,len(wndwng0d(ecg_test[i])))\n",
    "                maxpeaks=int(np.max(peaklens))\n",
    "                #print(maxpeaks)\n",
    "\n",
    "                prd_lbls_tst=np.empty((0,maxpeaks), int)              #have peaks(windows) lables for each raw signal\n",
    "\n",
    "                for i in range (len(ecg_test)):\n",
    "                    windws=np.array([])\n",
    "                    yyy=np.array([])\n",
    "                    yy=np.array([])\n",
    "                    win=np.array([])\n",
    "                    x=np.array(ecg_test[i,:-1])\n",
    "                    windws=wndwng0d(x)\n",
    "                    windws= 2*(windws-mn_aug)/(mx_aug - mn_aug) - 1\n",
    "                    windws = np.expand_dims(windws, axis=2)\n",
    "                    yy= model_crs.predict(windws)#yy= Predict each window lable for a raw signal (3600 Pnt) (17 prob in row)\n",
    "                    yyy=np.int16(maxindx(yy))         # one-hot to class lable\n",
    "                    maxcntlbl=np.append(maxcntlbl,(np.bincount(yyy).argmax()))\n",
    "                    zers=np.zeros(int(maxpeaks-len(yyy)))\n",
    "                    x=np.array(np.append(yyy, zers))\n",
    "                    prd_lbls_tst=np.append(prd_lbls_tst, [x], axis=0)\n",
    "                maxcntlbl=np.int16(maxcntlbl)                           \n",
    "                maxcntlbl=np.int16(maxcntlbl)\n",
    "\n",
    "                lbls=np.int16(ecg_test[:,-1])\n",
    "\n",
    "                '''cm = confusion_matrix(lbls, maxcntlbl)\n",
    "                print(cm)\n",
    "                acc3=accuracy_score(lbls, maxcntlbl)\n",
    "                print(classification_report(lbls, maxcntlbl))\n",
    "\n",
    "                predict_x=model_crs.predict(X_test)# Function 1\n",
    "                y_pred_crs=maxindx(predict_x)      # function from augment.py to remove 0 index predictions'''\n",
    "\n",
    "                rprt0=classification_report(lbls, maxcntlbl,output_dict=True)\n",
    "                rprt_row=np.array([])\n",
    "                rprt_row=np.append(rprt_row,rprt0['accuracy'])\n",
    "                rprt_row=np.append(rprt_row,rprt0['macro avg']['f1-score'])\n",
    "                rprt_row=np.append(rprt_row,rprt0['weighted avg']['f1-score'])\n",
    "                rprt=np.append(rprt,[rprt_row],axis=0)\n",
    "            #print('rprt=',rprt)\n",
    "            maxm=rprt[np.argmax(rprt[:,1])]\n",
    "            #print('maxm=',maxm)\n",
    "            avrg=np.mean(rprt,axis=0)\n",
    "            #print('avrg=',avrg)\n",
    "            rslts=np.append(rslts,['X '+str(X)+'  alpha '+str(alpha) +'  Beta '+str(beta)+'  average= '+str(np.int16(100*avrg))+'  max= '+str(np.int16(100*maxm))+'  var= '+str(np.int16(np.var(100*rprt,axis=0)))],axis=0)\n",
    "            print('rslts=',rslts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_wndws[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(f_scr_rtio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgScr[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os._exit(00)\n",
    "srt2"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
