{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time\n",
    "plc=1\n",
    "time.sleep(3000*plc)         #run after 1 houre = 3600 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#notebook { padding-top:0px !important; } .container { width:100% !important; } .end_space { min-height:0px !important; } html, body, .container{ margin:0!important;padding:0!important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import scipy\n",
    "import random\n",
    "import bisect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess import *                         #ماژول استخراج پنجره ها\n",
    "from data import *                          #ماژول محلی ورود داده ها\n",
    "from augment import *                            #ماژول های داده افزایی\n",
    "from lstm_cnn import *                           #ماژول های داده افزایی با lstm_cnn\n",
    "import augment\n",
    "import importlib\n",
    "importlib.reload(augment)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,recall_score,precision_score\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D,Conv1D,Dropout,MaxPooling1D,MaxPooling2D,Flatten,Dense\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "cwd = os.getcwd() #\n",
    "fullscrn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><div style=\"direction:rtl;font-family:B Nazanin\">Importing Data</div></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_train_shape= (1800, 751)\n",
      "first_test_shape= (1965, 751)\n",
      "classes_quantity= 42\n",
      "tr_lbls=\t {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42}\n",
      "Count_labels= [36 52 45 40 48 44 36 50 46 56 36 39 46 42 40 39 52 45 40 49 43 40 35 41\n",
      " 41 37 47 40 47 40 43 41 37 52 45 40 37 41 43 45 39 45]\n",
      "max(train_feature_Altitude)= 42.0\n",
      "min(train_feature_Altitude)= -5.7322181\n",
      "first_train_sample=\n",
      " [ 3.3000000e+01  3.0577079e+00  2.9885631e+00  2.8214720e+00\n",
      "  2.6392369e+00  2.4456015e+00  2.1598024e+00  1.7604417e+00\n",
      "  1.2885524e+00  7.7969192e-01  2.5702086e-01 -2.4136675e-01\n",
      " -6.9723052e-01 -1.1682231e+00 -1.7182241e+00 -2.3618027e+00\n",
      " -3.0698550e+00 -3.7349237e+00 -4.2844303e+00 -4.7071089e+00\n",
      " -5.0354394e+00 -5.2173834e+00 -5.2658047e+00 -5.2337997e+00\n",
      " -5.1379266e+00 -5.0083663e+00 -4.8910492e+00 -4.7745101e+00\n",
      " -4.5631708e+00 -4.2865916e+00 -4.0250031e+00 -3.7163384e+00\n",
      " -3.3250977e+00 -2.9576767e+00 -2.6556452e+00 -2.3643894e+00\n",
      " -2.0326444e+00 -1.6464083e+00 -1.2225265e+00 -8.2820868e-01\n",
      " -5.3175660e-01 -2.9191098e-01 -8.1817113e-02  4.4156439e-02\n",
      "  1.3046101e-01  1.7537985e-01  1.5271733e-01  1.1909526e-01\n",
      "  1.0086638e-01  9.4321304e-02  1.0088937e-01  1.1573459e-01\n",
      "  1.2371283e-01  1.4680454e-01  2.3097467e-01  3.0520840e-01\n",
      "  3.0520840e-01  3.0520840e-01  3.4728389e-01  3.6537865e-01\n",
      "  3.3194818e-01  3.0699795e-01  3.0981065e-01  3.0770304e-01\n",
      "  2.8372231e-01  2.6043135e-01  2.6618703e-01  3.0123077e-01\n",
      "  3.0543066e-01  2.4160467e-01  1.7777869e-01  2.1760095e-01\n",
      "  2.9706921e-01  3.6170758e-01  4.3673454e-01  4.8830575e-01\n",
      "  4.8911047e-01  4.7214233e-01  4.6980097e-01  4.6855173e-01\n",
      "  4.5791407e-01  4.4835703e-01  4.4495037e-01  4.2899388e-01\n",
      "  4.3122411e-01  4.6014430e-01  4.6376172e-01  4.7093141e-01\n",
      "  4.7103871e-01  4.5336547e-01  4.4544087e-01  4.5271020e-01\n",
      "  4.4989750e-01  4.3239287e-01  4.1196825e-01  4.3030442e-01\n",
      "  4.4660580e-01  4.7585938e-01  5.1518348e-01  5.0663043e-01\n",
      "  4.9067394e-01  4.9357094e-01  4.9751024e-01  4.8872727e-01\n",
      "  4.8154225e-01  4.8315936e-01  4.8399091e-01  4.9099582e-01\n",
      "  5.0776088e-01  4.9858704e-01  4.7465229e-01  5.0632387e-01\n",
      "  5.2117675e-01  5.3002103e-01  5.6218694e-01  5.9846842e-01\n",
      "  5.9478203e-01  5.6967852e-01  5.5074838e-01  5.3479189e-01\n",
      "  5.3160365e-01  5.6301848e-01  6.0290973e-01  6.4113787e-01\n",
      "  6.4379729e-01  6.3875436e-01  6.3107884e-01  6.2524652e-01\n",
      "  6.1218703e-01  5.7036445e-01  5.4327596e-01  5.5489462e-01\n",
      "  5.7650333e-01  5.9595462e-01  6.2648043e-01  6.2654174e-01\n",
      "  5.8099445e-01  5.8403707e-01  6.1329065e-01  6.3508330e-01\n",
      "  6.6318727e-01  6.9037539e-01  6.9100384e-01  6.8694191e-01\n",
      "  7.0557315e-01  7.2792144e-01  7.3488804e-01  7.4501221e-01\n",
      "  7.5920599e-01  7.4629211e-01  7.2239568e-01  7.3835218e-01\n",
      "  7.3598783e-01  7.3332841e-01  7.4823494e-01  7.3509114e-01\n",
      "  7.0583373e-01  7.0362649e-01  7.2088970e-01  7.3684620e-01\n",
      "  7.6397684e-01  8.0536257e-01  8.2471039e-01  8.2411260e-01\n",
      "  8.1782427e-01  7.8325186e-01  7.5121624e-01  7.8210609e-01\n",
      "  8.1565152e-01  8.0947431e-01  7.9497398e-01  7.9323425e-01\n",
      "  7.9473640e-01  8.0163019e-01  8.2205481e-01  8.4131067e-01\n",
      "  8.5872333e-01  8.5715604e-01  8.6774772e-01  8.6242888e-01\n",
      "  8.7921693e-01  9.3151622e-01  9.4525782e-01  9.2154533e-01\n",
      "  9.0310569e-01  8.9933499e-01  9.1125255e-01  9.3686571e-01\n",
      "  9.5001718e-01  9.4567167e-01  9.7945852e-01  1.0403875e+00\n",
      "  1.0483658e+00  1.0278913e+00  1.0398778e+00  1.0450089e+00\n",
      "  1.0398510e+00  1.0650005e+00  1.1060184e+00  1.1353333e+00\n",
      "  1.1472815e+00  1.1445378e+00  1.1284816e+00  1.1136479e+00\n",
      "  1.1391691e+00  1.1544933e+00  1.1411962e+00  1.1379237e+00\n",
      "  1.1260828e+00  1.1065319e+00  1.1152382e+00  1.1481552e+00\n",
      "  1.1691661e+00  1.1831759e+00  1.2174188e+00  1.2507266e+00\n",
      "  1.2454078e+00  1.2460669e+00  1.2817659e+00  1.3322948e+00\n",
      "  1.4065516e+00  1.4519072e+00  1.4213661e+00  1.3941933e+00\n",
      "  1.3782981e+00  1.3286008e+00  1.2829462e+00  1.3084521e+00\n",
      "  1.3813906e+00  1.4342800e+00  1.4763172e+00  1.4969832e+00\n",
      "  1.4542179e+00  1.4063485e+00  1.3865944e+00  1.3843949e+00\n",
      "  1.3972014e+00  1.3977686e+00  1.3855981e+00  1.3666373e+00\n",
      "  1.3531103e+00  1.3499527e+00  1.3577815e+00  1.3810572e+00\n",
      "  1.3965040e+00  1.3817814e+00  1.3528497e+00  1.3555092e+00\n",
      "  1.3457912e+00  1.3193005e+00  1.3229102e+00  1.3251481e+00\n",
      "  1.3257536e+00  1.3156179e+00  1.3100845e+00  1.3253819e+00\n",
      "  1.3313675e+00  1.3022250e+00  1.2517804e+00  1.1889086e+00\n",
      "  1.1835898e+00  1.1704536e+00  1.1451815e+00  1.1579038e+00\n",
      "  1.1811526e+00  1.1416293e+00  1.0605132e+00  9.9287056e-01\n",
      "  9.4726196e-01  9.0287194e-01  8.5854706e-01  8.2447281e-01\n",
      "  7.9457162e-01  7.8158110e-01  7.7892169e-01  7.2575636e-01\n",
      "  6.6759408e-01  6.4485109e-01  6.3344703e-01  6.2449162e-01\n",
      "  6.0326993e-01  5.5244596e-01  4.7434190e-01  4.1343591e-01\n",
      "  4.0512044e-01  3.7977935e-01  3.6364658e-01  3.7694366e-01\n",
      "  3.4244023e-01  2.9564370e-01  2.6788078e-01  2.4394603e-01\n",
      "  2.0007333e-01  1.5329213e-01  1.0656841e-01  6.0760546e-02\n",
      "  4.1228782e-02  5.8702755e-02  7.5594263e-02  7.5931480e-02\n",
      "  7.7452789e-02  5.6177460e-02 -3.7201001e-02 -1.1166082e-01\n",
      " -1.4470042e-01 -1.7252083e-01 -1.8098957e-01 -1.8137278e-01\n",
      " -2.0631151e-01 -2.4318312e-01 -2.5023403e-01 -2.3370656e-01\n",
      " -2.3057964e-01 -2.2885523e-01 -2.6200980e-01 -3.6572703e-01\n",
      " -4.2703538e-01 -4.4443271e-01 -4.2811984e-01 -4.2280867e-01\n",
      " -4.4033246e-01 -4.7845331e-01 -4.8696038e-01 -4.4361266e-01\n",
      " -4.0093555e-01 -3.8710199e-01 -3.9247064e-01 -4.1815278e-01\n",
      " -5.0591351e-01 -6.0154902e-01 -6.1520631e-01 -6.2493196e-01\n",
      " -6.0169464e-01 -5.3148452e-01 -4.5000829e-01 -3.9737178e-01\n",
      " -3.8306305e-01 -3.9964799e-01 -4.4561681e-01 -5.0389787e-01\n",
      " -5.1596871e-01 -5.1881972e-01 -5.2679797e-01 -5.1696503e-01\n",
      " -5.2966432e-01 -5.4774375e-01 -5.5871097e-01 -5.8880759e-01\n",
      " -6.3881151e-01 -6.6650162e-01 -6.4566697e-01 -6.0294771e-01\n",
      " -5.6585384e-01 -5.1921825e-01 -5.0863424e-01 -5.2689760e-01\n",
      " -4.8434695e-01 -4.7875604e-01 -5.7298521e-01 -6.7038345e-01\n",
      " -6.9752176e-01 -6.6244352e-01 -5.8826344e-01 -5.0983749e-01\n",
      " -4.7764859e-01 -5.0670290e-01 -5.4719960e-01 -5.6085306e-01\n",
      " -5.7724641e-01 -6.0118115e-01 -5.6804191e-01 -5.3108599e-01\n",
      " -5.1123617e-01 -5.0705162e-01 -5.0880285e-01 -5.0221945e-01\n",
      " -4.7163616e-01 -4.4390773e-01 -4.4390773e-01 -4.3693347e-01\n",
      " -4.1569263e-01 -4.2304242e-01 -4.7097706e-01 -5.2150596e-01\n",
      " -5.4941067e-01 -5.5758052e-01 -5.4616113e-01 -5.2079704e-01\n",
      " -4.8653119e-01 -4.5367936e-01 -4.4816126e-01 -4.7566744e-01\n",
      " -4.9965583e-01 -4.9988959e-01 -5.1891169e-01 -5.3471491e-01\n",
      " -4.8152659e-01 -4.4155870e-01 -4.1870076e-01 -4.2187366e-01\n",
      " -4.4594636e-01 -5.0401666e-01 -5.3129292e-01 -5.2449493e-01\n",
      " -5.1736356e-01 -5.0416611e-01 -4.6478070e-01 -4.1700317e-01\n",
      " -4.1722160e-01 -4.5791756e-01 -5.1642472e-01 -5.6467741e-01\n",
      " -5.4901981e-01 -4.9723783e-01 -4.4452468e-01 -3.8539291e-01\n",
      " -3.5025336e-01 -3.6813353e-01 -4.3389851e-01 -4.9972098e-01\n",
      " -5.1832923e-01 -5.2486281e-01 -5.0788699e-01 -4.5518151e-01\n",
      " -4.3922501e-01 -4.3071794e-01 -4.3864638e-01 -4.6165377e-01\n",
      " -4.7529190e-01 -4.6686914e-01 -4.5254125e-01 -4.5652654e-01\n",
      " -4.5966113e-01 -4.3864638e-01 -4.3535851e-01 -4.4377361e-01\n",
      " -4.2266306e-01 -4.0404714e-01 -4.1156555e-01 -4.2194647e-01\n",
      " -4.6984661e-01 -5.3361895e-01 -5.6646696e-01 -5.4031348e-01\n",
      " -4.8581461e-01 -4.2921196e-01 -3.8288294e-01 -3.8074851e-01\n",
      " -4.1773509e-01 -4.3103217e-01 -4.4491938e-01 -4.7683237e-01\n",
      " -4.7075097e-01 -4.4651733e-01 -4.4314899e-01 -4.4960976e-01\n",
      " -4.5589809e-01 -4.5382114e-01 -4.3442733e-01 -4.1520596e-01\n",
      " -4.2367471e-01 -4.4454384e-01 -4.4294973e-01 -4.2051713e-01\n",
      " -4.1519830e-01 -4.0987946e-01 -3.9475068e-01 -3.7655246e-01\n",
      " -3.4575075e-01 -3.2209957e-01 -3.1635922e-01 -3.2041732e-01\n",
      " -3.1979653e-01 -3.2549857e-01 -3.5092013e-01 -3.6187585e-01\n",
      " -3.5286296e-01 -3.3784531e-01 -3.4050472e-01 -3.4058137e-01\n",
      " -3.1686121e-01 -3.2700838e-01 -3.3197083e-01 -3.3197083e-01\n",
      " -3.1668111e-01 -3.0986013e-01 -3.3976898e-01 -3.8847385e-01\n",
      " -4.1876207e-01 -4.2284699e-01 -4.3533552e-01 -4.6394532e-01\n",
      " -4.6128590e-01 -4.4361266e-01 -4.3187138e-01 -4.1291059e-01\n",
      " -3.8789521e-01 -3.7789366e-01 -3.6630566e-01 -3.6112095e-01\n",
      " -3.8776876e-01 -4.3155715e-01 -4.5825861e-01 -4.5944270e-01\n",
      " -4.6686914e-01 -4.6686914e-01 -4.2738026e-01 -4.3108198e-01\n",
      " -4.2358274e-01 -3.6493763e-01 -3.4237091e-01 -3.7660611e-01\n",
      " -4.2424568e-01 -4.6529419e-01 -5.0027279e-01 -4.9800424e-01\n",
      " -4.3837814e-01 -3.9614170e-01 -3.5542275e-01 -2.9425618e-01\n",
      " -3.0454896e-01 -3.6467322e-01 -3.9706905e-01 -4.0372525e-01\n",
      " -4.0714341e-01 -4.0047571e-01 -3.8194410e-01 -3.3267209e-01\n",
      " -2.5264436e-01 -2.2755618e-01 -3.0956889e-01 -4.1080298e-01\n",
      " -4.5823562e-01 -4.5557620e-01 -4.7268997e-01 -4.8904499e-01\n",
      " -4.9680099e-01 -4.6717187e-01 -4.1544738e-01 -3.8086730e-01\n",
      " -3.7215331e-01 -3.5820479e-01 -3.2647190e-01 -3.1820242e-01\n",
      " -3.5182449e-01 -4.1314434e-01 -4.7697032e-01 -5.4079631e-01\n",
      " -5.3527821e-01 -4.7619243e-01 -4.0704761e-01 -3.4669726e-01\n",
      " -3.0942711e-01 -2.9886609e-01 -3.0039889e-01 -3.0039889e-01\n",
      " -3.1520578e-01 -3.7006486e-01 -4.1605667e-01 -4.2396211e-01\n",
      " -4.2928094e-01 -4.3207831e-01 -4.2318421e-01 -4.2304626e-01\n",
      " -4.2192731e-01 -4.0416210e-01 -3.9371221e-01 -3.9134402e-01\n",
      " -3.7708894e-01 -3.6067260e-01 -3.6879264e-01 -4.0129576e-01\n",
      " -4.2175870e-01 -4.3239636e-01 -4.2712352e-01 -3.8518981e-01\n",
      " -3.6005182e-01 -3.5493225e-01 -3.6216709e-01 -3.8558834e-01\n",
      " -4.2305009e-01 -4.4560148e-01 -4.3521673e-01 -4.1128198e-01\n",
      " -3.7755261e-01 -3.0841546e-01 -2.5923542e-01 -2.3796009e-01\n",
      " -2.2439860e-01 -2.3470671e-01 -2.5015355e-01 -2.3314709e-01\n",
      " -1.9870880e-01 -1.7968669e-01 -1.3385966e-01 -4.3907021e-02\n",
      "  2.3501896e-02  4.1481695e-02  7.1907868e-02  1.3459191e-01\n",
      "  2.1163068e-01  2.8343491e-01  3.6271157e-01  3.9917316e-01\n",
      "  4.0566459e-01  4.1124017e-01  4.2086618e-01  4.3437785e-01\n",
      "  4.4910810e-01  4.5935874e-01  4.6226340e-01  4.6316776e-01\n",
      "  4.7186642e-01  4.5359539e-01  4.4029831e-01  4.6369657e-01\n",
      "  4.5870729e-01  4.0453798e-01  3.9838760e-01  4.4724575e-01\n",
      "  5.1373115e-01  5.8699921e-01  6.4959512e-01  6.7920125e-01\n",
      "  6.9056316e-01  6.9050951e-01  6.3012084e-01  5.3612926e-01\n",
      "  4.9091918e-01  4.7873722e-01  4.8673846e-01  5.2068625e-01\n",
      "  5.3958573e-01  5.1287661e-01  4.5757302e-01  3.8236979e-01\n",
      "  2.9301494e-01  2.2661768e-01  1.9954834e-01  1.4784301e-01\n",
      "  6.5853289e-02  1.7983799e-02 -2.9885690e-02 -9.4976241e-02\n",
      " -1.6146164e-01 -2.4017883e-01 -3.0799777e-01 -3.1579975e-01\n",
      " -2.6670401e-01 -2.0353713e-01 -1.5359836e-01 -1.1680338e-01\n",
      " -9.0366331e-02 -7.7582740e-02 -8.7545970e-02 -1.3275604e-01\n",
      " -1.9868964e-01 -2.3185953e-01 -2.6545094e-01 -3.2547941e-01\n",
      " -3.5792888e-01 -3.5585960e-01 -3.5714715e-01 -3.6418273e-01\n",
      " -3.6067260e-01 -3.6483417e-01 -3.8851600e-01 -4.4430243e-01\n",
      " -4.7572492e-01 -4.8636259e-01 -4.6939061e-01 -4.6227073e-01\n",
      " -4.5695190e-01 -4.7604298e-01 -4.9529500e-01 -4.9694277e-01\n",
      " -4.9892775e-01 -4.9877447e-01 -4.7420361e-01 -4.4842184e-01\n",
      " -4.3787231e-01 -3.9893907e-01 -3.7234491e-01 -3.4839100e-01\n",
      " -3.0502030e-01 -2.6424769e-01 -2.6652391e-01 -3.0045254e-01\n",
      " -3.4934901e-01 -4.1766994e-01 -4.7098855e-01 -4.8694505e-01\n",
      " -5.0661860e-01 -5.2894773e-01 -5.1719112e-01 -4.5868396e-01\n",
      " -4.0017681e-01 -3.7750663e-01 -4.1025883e-01 -4.5870313e-01\n",
      " -4.9188451e-01 -5.0474475e-01 -5.1458152e-01 -5.1233596e-01\n",
      " -4.7636870e-01 -4.2743391e-01 -4.0482504e-01 -3.8937437e-01\n",
      " -3.9227137e-01 -4.2406174e-01 -4.4533707e-01 -4.4181928e-01\n",
      " -4.6372306e-01 -4.5945037e-01 -4.3137705e-01 -4.1643987e-01\n",
      " -4.0522740e-01 -3.9389998e-01 -4.0228442e-01 -4.2887858e-01\n",
      " -4.4557082e-01 -4.2472468e-01 -3.9063510e-01 -3.6404094e-01\n",
      " -3.3483718e-01 -3.3916735e-01 -3.4495752e-01 -3.2259390e-01\n",
      " -2.9598058e-01 -2.9538279e-01 -2.9788892e-01 -2.6456575e-01\n",
      " -1.9519867e-01 -1.1042691e-01 -1.9886455e-03  1.4988931e-01\n",
      "  3.0333072e-01  4.3098269e-01  5.5610553e-01  6.7182462e-01\n",
      "  8.0514797e-01  9.5861238e-01  1.1266078e+00  1.3201244e+00\n",
      "  1.5226308e+00  1.7262525e+00  1.9814184e+00  2.3291505e+00\n",
      "  2.6963492e+00  2.9390075e+00  3.0347465e+00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_4776/3524553684.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vars()['ecg'+str(i)],vars()['ecg_tst'+str(i)]=np.array(NonInvasiveFetalECGThorax1(i))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 36\t2 52\t3 45\t4 40\t5 48\t6 44\t7 36\t8 50\t9 46\t10 56\t11 36\t12 39\t13 46\t14 42\t15 40\t16 39\t17 52\t18 45\t19 40\t20 49\t21 43\t22 40\t23 35\t24 41\t25 41\t26 37\t27 47\t28 40\t29 47\t30 40\t31 43\t32 41\t33 37\t34 52\t35 45\t36 40\t37 37\t38 41\t39 43\t40 45\t41 39\t42 45\t"
     ]
    }
   ],
   "source": [
    "rate=480\n",
    "cls_num=42\n",
    "for i in range (1,cls_num+1):\n",
    "    vars()['ecg'+str(i)],vars()['ecg_tst'+str(i)]=np.array(NonInvasiveFetalECGThorax1(i))\n",
    "\n",
    "os.chdir(cwd)\n",
    "clses_lens=np.array([])\n",
    "i=0                               #جمع آوری داده ها و چاپ تعداد نمونه ی هر کلاس\n",
    "ecg=np.array(ecg1)\n",
    "print(1,len(vars()['ecg'+str(1)]), end='\\t')\n",
    "clses_lens=np.append(clses_lens,len(vars()['ecg'+str(1)]))\n",
    "for i in range (2,cls_num+1):\n",
    "    ecg=np.concatenate((ecg,vars()['ecg'+str(i)]),axis=0)\n",
    "    clses_lens=np.append(clses_lens,len(vars()['ecg'+str(i)]))\n",
    "    print(i,len(vars()['ecg'+str(i)]), end='\\t')\n",
    "\n",
    "mx_sig=max(clses_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 481)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(ecg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center><div style=\"direction:rtl;font-family:B Nazanin\">Base Train windows</div></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each class and its windows =\n",
      "1 36\t2 52\t3 45\t4 40\t5 48\t6 44\t7 36\t8 50\t9 46\t10 56\t11 36\t12 39\t13 46\t14 42\t15 40\t16 39\t17 52\t18 45\t19 40\t20 49\t21 43\t22 40\t23 35\t24 41\t25 41\t26 37\t27 47\t28 40\t29 47\t30 40\t31 43\t32 41\t33 37\t34 52\t35 45\t36 40\t37 37\t38 41\t39 43\t40 45\t41 39\t42 45\t\n",
      " max = 56\n"
     ]
    }
   ],
   "source": [
    "smpl_rte=480                                              # در ماژول ها نیز همین مقدار ثبت شده\n",
    "i=0\n",
    "windws=np.array([])\n",
    "\n",
    "for cls in range (1,cls_num+1):                                 #ساخت پنجره های داده های آموزش اصلی\n",
    "    vars()['wndws'+str(cls)]=np.array(vars()['ecg'+str(cls)])  \n",
    "    \n",
    "print(\"each class and its windows =\")\n",
    "\n",
    "cls_wndws=np.array([])\n",
    "for cls in range (1,cls_num+1):                                # آرایه ی تعداد پنجره ی هر کلاس\n",
    "    wns=len(vars()['wndws'+str(cls)])\n",
    "    cls_wndws=np.append(cls_wndws,wns)\n",
    "    print(cls, wns, end='\\t')\n",
    "\n",
    "mx_wndws=int(np.max(cls_wndws))                        # حداکثر تعداد پنجره ی موجود بین کلاس ها\n",
    "print('\\n max =', mx_wndws)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#clses=3                                   #تعیین میزان افزایش نمونه (تولید داده)\n",
    "mx_wndws=int(mx_wndws)  #mx_wndws*0.3  # =classes-1 برای بررسی الگوریتم تعداد کمی کلاس آزمایش شد\n",
    "mx_wndws=2598           #+2\n",
    "print(mx_wndws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><div style=\"direction:rtl;font-family:B Nazanin\">Data Augmentation</div></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up to class  1 train shape =  (36, 481)\n",
      "Up to class  2 train shape =  (88, 481)\n",
      "Up to class  3 train shape =  (133, 481)\n",
      "Up to class  4 train shape =  (173, 481)\n",
      "Up to class  5 train shape =  (221, 481)\n",
      "Up to class  6 train shape =  (265, 481)\n",
      "Up to class  7 train shape =  (301, 481)\n",
      "Up to class  8 train shape =  (351, 481)\n",
      "Up to class  9 train shape =  (397, 481)\n",
      "Up to class  10 train shape =  (453, 481)\n",
      "Up to class  11 train shape =  (489, 481)\n",
      "Up to class  12 train shape =  (528, 481)\n",
      "Up to class  13 train shape =  (574, 481)\n",
      "Up to class  14 train shape =  (616, 481)\n",
      "Up to class  15 train shape =  (656, 481)\n",
      "Up to class  16 train shape =  (695, 481)\n",
      "Up to class  17 train shape =  (747, 481)\n",
      "Up to class  18 train shape =  (792, 481)\n",
      "Up to class  19 train shape =  (832, 481)\n",
      "Up to class  20 train shape =  (881, 481)\n",
      "Up to class  21 train shape =  (924, 481)\n",
      "Up to class  22 train shape =  (964, 481)\n",
      "Up to class  23 train shape =  (999, 481)\n",
      "Up to class  24 train shape =  (1040, 481)\n",
      "Up to class  25 train shape =  (1081, 481)\n",
      "Up to class  26 train shape =  (1118, 481)\n",
      "Up to class  27 train shape =  (1165, 481)\n",
      "Up to class  28 train shape =  (1205, 481)\n",
      "Up to class  29 train shape =  (1252, 481)\n",
      "Up to class  30 train shape =  (1292, 481)\n",
      "Up to class  31 train shape =  (1335, 481)\n",
      "Up to class  32 train shape =  (1376, 481)\n",
      "Up to class  33 train shape =  (1413, 481)\n",
      "Up to class  34 train shape =  (1465, 481)\n",
      "Up to class  35 train shape =  (1510, 481)\n",
      "Up to class  36 train shape =  (1550, 481)\n",
      "Up to class  37 train shape =  (1587, 481)\n",
      "Up to class  38 train shape =  (1628, 481)\n",
      "Up to class  39 train shape =  (1671, 481)\n",
      "Up to class  40 train shape =  (1716, 481)\n",
      "Up to class  41 train shape =  (1755, 481)\n",
      "Up to class  42 train shape =  (1800, 481)\n"
     ]
    }
   ],
   "source": [
    "smpl_rte=len(wndws1[0])                                        # در ماژول ها نیز همین مقدار ثبت شده\n",
    "\n",
    "#add augmented data to base data\n",
    "xtrain=np.empty((0,len(wndws1[0])), float) \n",
    "for i in range (1,cls_num+1):   \n",
    "    xtrain=np.append(xtrain,vars()['wndws'+str(i)],axis=0)\n",
    "    print('Up to class ', i, 'train shape = ', np.shape(xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 36 52 45 40 48 44 36 50 46 56 36 39 46 42 40 39 52 45 40 49 43 40 35\n",
      " 41 41 37 47 40 47 40 43 41 37 52 45 40 37 41 43 45 39 45]\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(np.int16(xtrain[:,-1])))    #تعداد پنجره در هر کلاس از 0 تا 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ..., 42., 42., 42.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_aug=np.max(xtrain[:,:-1])\n",
    "mx=np.max(ecg[:,:-1])\n",
    "mn_aug=np.min(xtrain[:,:-1])\n",
    "mn=np.min(ecg[:,:-1])\n",
    "\n",
    "#for i in range (1,cls_num+1):             #نرمالسازی داده های افزایشی\n",
    "xtrain[:,:-1]= 2*(xtrain[:,:-1]-mn_aug)/(mx_aug - mn_aug) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min trn = -1.0\n",
      "max trn = 1.0\n"
     ]
    }
   ],
   "source": [
    "print('min trn =', np.min(xtrain[:,:-1]))\n",
    "print('max trn =', np.max(xtrain[:,:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center><div style=\"direction:rtl;font-family:B Nazanin\">Test Windows</div></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wndws_test=np.empty((0,len(wndws1[0])), float)\n",
    "i=0\n",
    "windws=np.array([])\n",
    "for cls in range (1,cls_num+1):                                 #ساخت پنجره های داده های آموزش اصلی\n",
    "    vars()['wndws_tst'+str(cls)]=np.empty((0,len(wndws1[0])), float)\n",
    "    vars()['wndws_tst'+str(cls)]=np.append(vars()['wndws_tst'+str(cls)],vars()['ecg_tst'+str(cls)],axis=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cls 1  >> \n",
      "max magnitude class 1  =  3.464960350576406\n",
      "min magnitude class 1  =  -4.0359667553513825\n",
      "after normalizing >>\n",
      "max magnitude class 1  =  0.7551004095379115\n",
      "min magnitude class 1  =  -0.6763971402363305\n",
      "\n",
      " cls 2  >> \n",
      "max magnitude class 2  =  4.425310812468809\n",
      "min magnitude class 2  =  -3.0005668376799934\n",
      "after normalizing >>\n",
      "max magnitude class 2  =  0.9383763317951899\n",
      "min magnitude class 2  =  -0.47879857351892796\n",
      "\n",
      " cls 3  >> \n",
      "max magnitude class 3  =  3.498304696858196\n",
      "min magnitude class 3  =  -4.053376157408748\n",
      "after normalizing >>\n",
      "max magnitude class 3  =  0.7614639362496272\n",
      "min magnitude class 3  =  -0.6797195983853317\n",
      "\n",
      " cls 4  >> \n",
      "max magnitude class 4  =  3.6561362518347744\n",
      "min magnitude class 4  =  -3.8223670751843044\n",
      "after normalizing >>\n",
      "max magnitude class 4  =  0.7915849440977742\n",
      "min magnitude class 4  =  -0.6356331900640786\n",
      "\n",
      " cls 5  >> \n",
      "max magnitude class 5  =  4.072548295052668\n",
      "min magnitude class 5  =  -3.356934300988409\n",
      "after normalizing >>\n",
      "max magnitude class 5  =  0.8710541631882818\n",
      "min magnitude class 5  =  -0.5468087199103133\n",
      "\n",
      " cls 6  >> \n",
      "max magnitude class 6  =  4.246257266174673\n",
      "min magnitude class 6  =  -2.8165648738408784\n",
      "after normalizing >>\n",
      "max magnitude class 6  =  0.9042052607823969\n",
      "min magnitude class 6  =  -0.44368313289810324\n",
      "\n",
      " cls 7  >> \n",
      "max magnitude class 7  =  3.382102981160317\n",
      "min magnitude class 7  =  -4.877851445235264\n",
      "after normalizing >>\n",
      "max magnitude class 7  =  0.739287681374464\n",
      "min magnitude class 7  =  -0.8370647288879814\n",
      "\n",
      " cls 8  >> \n",
      "max magnitude class 8  =  4.1262970068705975\n",
      "min magnitude class 8  =  -2.806936368337397\n",
      "after normalizing >>\n",
      "max magnitude class 8  =  0.8813117151127743\n",
      "min magnitude class 8  =  -0.4418456024379913\n",
      "\n",
      " cls 9  >> \n",
      "max magnitude class 9  =  5.200288095823055\n",
      "min magnitude class 9  =  -4.651582005712916\n",
      "after normalizing >>\n",
      "max magnitude class 9  =  1.0862751270488578\n",
      "min magnitude class 9  =  -0.7938828470097987\n",
      "\n",
      " cls 10  >> \n",
      "max magnitude class 10  =  4.009981924386002\n",
      "min magnitude class 10  =  -3.1154149352887317\n",
      "after normalizing >>\n",
      "max magnitude class 10  =  0.8591138250083761\n",
      "min magnitude class 10  =  -0.5007165001967139\n",
      "\n",
      " cls 11  >> \n",
      "max magnitude class 11  =  3.7631360911170617\n",
      "min magnitude class 11  =  -3.6180304901367504\n",
      "after normalizing >>\n",
      "max magnitude class 11  =  0.8120050875808096\n",
      "min magnitude class 11  =  -0.5966370344297106\n",
      "\n",
      " cls 12  >> \n",
      "max magnitude class 12  =  3.8589458299015265\n",
      "min magnitude class 12  =  -4.39998552466251\n",
      "after normalizing >>\n",
      "max magnitude class 12  =  0.8302896815220149\n",
      "min magnitude class 12  =  -0.7458674828995606\n",
      "\n",
      " cls 13  >> \n",
      "max magnitude class 13  =  4.105977961847347\n",
      "min magnitude class 13  =  -3.151600682115612\n",
      "after normalizing >>\n",
      "max magnitude class 13  =  0.8774339727012332\n",
      "min magnitude class 13  =  -0.5076222875997796\n",
      "\n",
      " cls 14  >> \n",
      "max magnitude class 14  =  4.279654186131015\n",
      "min magnitude class 14  =  -3.3136273289888147\n",
      "after normalizing >>\n",
      "max magnitude class 14  =  0.9105788207986953\n",
      "min magnitude class 14  =  -0.5385438983182504\n",
      "\n",
      " cls 15  >> \n",
      "max magnitude class 15  =  4.384530971016013\n",
      "min magnitude class 15  =  -3.504646921999563\n",
      "after normalizing >>\n",
      "max magnitude class 15  =  0.9305937947440397\n",
      "min magnitude class 15  =  -0.5749986025824292\n",
      "\n",
      " cls 16  >> \n",
      "max magnitude class 16  =  3.262280002852989\n",
      "min magnitude class 16  =  -4.829490079367857\n",
      "after normalizing >>\n",
      "max magnitude class 16  =  0.7164203347876614\n",
      "min magnitude class 16  =  -0.8278353128750969\n",
      "\n",
      " cls 17  >> \n",
      "max magnitude class 17  =  3.5083590791842023\n",
      "min magnitude class 17  =  -3.806020111805493\n",
      "after normalizing >>\n",
      "max magnitude class 17  =  0.7633827422133326\n",
      "min magnitude class 17  =  -0.6325134906332543\n",
      "\n",
      " cls 18  >> \n",
      "max magnitude class 18  =  3.924044172256362\n",
      "min magnitude class 18  =  -3.3087566691038335\n",
      "after normalizing >>\n",
      "max magnitude class 18  =  0.8427132281395615\n",
      "min magnitude class 18  =  -0.5376143681958174\n",
      "\n",
      " cls 19  >> \n",
      "max magnitude class 19  =  3.7743063544924262\n",
      "min magnitude class 19  =  -3.3216745079488637\n",
      "after normalizing >>\n",
      "max magnitude class 19  =  0.8141368513516545\n",
      "min magnitude class 19  =  -0.540079644073572\n",
      "\n",
      " cls 20  >> \n",
      "max magnitude class 20  =  4.408436118447938\n",
      "min magnitude class 20  =  -2.6717662362348324\n",
      "after normalizing >>\n",
      "max magnitude class 20  =  0.9351559187978451\n",
      "min magnitude class 20  =  -0.41604936282988114\n",
      "\n",
      " cls 21  >> \n",
      "max magnitude class 21  =  3.7971866714694595\n",
      "min magnitude class 21  =  -4.2514638676185745\n",
      "after normalizing >>\n",
      "max magnitude class 21  =  0.818503393944076\n",
      "min magnitude class 21  =  -0.717523201566819\n",
      "\n",
      " cls 22  >> \n",
      "max magnitude class 22  =  4.640160562461768\n",
      "min magnitude class 22  =  -2.9882587535190472\n",
      "after normalizing >>\n",
      "max magnitude class 22  =  0.9793788487295934\n",
      "min magnitude class 22  =  -0.4764496649013671\n",
      "\n",
      " cls 23  >> \n",
      "max magnitude class 23  =  5.576570633972081\n",
      "min magnitude class 23  =  -4.7658733083294145\n",
      "after normalizing >>\n",
      "max magnitude class 23  =  1.1580859210715708\n",
      "min magnitude class 23  =  -0.8156945134005744\n",
      "\n",
      " cls 24  >> \n",
      "max magnitude class 24  =  4.3677105440511745\n",
      "min magnitude class 24  =  -4.997450567246932\n",
      "after normalizing >>\n",
      "max magnitude class 24  =  0.9273837382208543\n",
      "min magnitude class 24  =  -0.8598893541239143\n",
      "\n",
      " cls 25  >> \n",
      "max magnitude class 25  =  3.77288854390894\n",
      "min magnitude class 25  =  -3.9759508855952834\n",
      "after normalizing >>\n",
      "max magnitude class 25  =  0.8138662724821841\n",
      "min magnitude class 25  =  -0.6649435466618068\n",
      "\n",
      " cls 26  >> \n",
      "max magnitude class 26  =  4.766055798553003\n",
      "min magnitude class 26  =  -3.068986299727066\n",
      "after normalizing >>\n",
      "max magnitude class 26  =  1.0034050416851854\n",
      "min magnitude class 26  =  -0.4918559317486667\n",
      "\n",
      " cls 27  >> \n",
      "max magnitude class 27  =  3.6868554087238388\n",
      "min magnitude class 27  =  -3.9137058510339813\n",
      "after normalizing >>\n",
      "max magnitude class 27  =  0.7974474724488465\n",
      "min magnitude class 27  =  -0.6530645331476372\n",
      "\n",
      " cls 28  >> \n",
      "max magnitude class 28  =  3.757162941979138\n",
      "min magnitude class 28  =  -4.175196031233502\n",
      "after normalizing >>\n",
      "max magnitude class 28  =  0.8108651553784798\n",
      "min magnitude class 28  =  -0.7029680380023665\n",
      "\n",
      " cls 29  >> \n",
      "max magnitude class 29  =  4.146750047868562\n",
      "min magnitude class 29  =  -3.258197075943474\n",
      "after normalizing >>\n",
      "max magnitude class 29  =  0.8852150296844996\n",
      "min magnitude class 29  =  -0.5279654364431758\n",
      "\n",
      " cls 30  >> \n",
      "max magnitude class 30  =  3.5345969275278595\n",
      "min magnitude class 30  =  -4.164649864625463\n",
      "after normalizing >>\n",
      "max magnitude class 30  =  0.7683900453219856\n",
      "min magnitude class 30  =  -0.7009553785743088\n",
      "\n",
      " cls 31  >> \n",
      "max magnitude class 31  =  4.034061890440679\n",
      "min magnitude class 31  =  -2.913544248966841\n",
      "after normalizing >>\n",
      "max magnitude class 31  =  0.8637093119287429\n",
      "min magnitude class 31  =  -0.46219094345418\n",
      "\n",
      " cls 32  >> \n",
      "max magnitude class 32  =  4.2167418596210915\n",
      "min magnitude class 32  =  -3.2132648063250064\n",
      "after normalizing >>\n",
      "max magnitude class 32  =  0.8985724594545912\n",
      "min magnitude class 32  =  -0.5193904385854042\n",
      "\n",
      " cls 33  >> \n",
      "max magnitude class 33  =  3.305581826067976\n",
      "min magnitude class 33  =  -5.739297335705209\n",
      "after normalizing >>\n",
      "max magnitude class 33  =  0.7246841737715148\n",
      "min magnitude class 33  =  -1.0014654308353825\n",
      "\n",
      " cls 34  >> \n",
      "max magnitude class 34  =  4.302909361127427\n",
      "min magnitude class 34  =  -3.0046204529950677\n",
      "after normalizing >>\n",
      "max magnitude class 34  =  0.9150169023260608\n",
      "min magnitude class 34  =  -0.479572176609494\n",
      "\n",
      " cls 35  >> \n",
      "max magnitude class 35  =  4.171240786187077\n",
      "min magnitude class 35  =  -3.0293974705036697\n",
      "after normalizing >>\n",
      "max magnitude class 35  =  0.8898889095129561\n",
      "min magnitude class 35  =  -0.48430069074559956\n",
      "\n",
      " cls 36  >> \n",
      "max magnitude class 36  =  3.525520872508194\n",
      "min magnitude class 36  =  -3.7778628727989307\n",
      "after normalizing >>\n",
      "max magnitude class 36  =  0.7666579460309526\n",
      "min magnitude class 36  =  -0.6271398857338748\n",
      "\n",
      " cls 37  >> \n",
      "max magnitude class 37  =  3.7898470172717285\n",
      "min magnitude class 37  =  -3.122089539810896\n",
      "after normalizing >>\n",
      "max magnitude class 37  =  0.8171026741594591\n",
      "min magnitude class 37  =  -0.5019903000729504\n",
      "\n",
      " cls 38  >> \n",
      "max magnitude class 38  =  3.8169094842144755\n",
      "min magnitude class 38  =  -2.7632509932329072\n",
      "after normalizing >>\n",
      "max magnitude class 38  =  0.8222673497485304\n",
      "min magnitude class 38  =  -0.43350856535711835\n",
      "\n",
      " cls 39  >> \n",
      "max magnitude class 39  =  3.297751214427255\n",
      "min magnitude class 39  =  -4.8411170786586855\n",
      "after normalizing >>\n",
      "max magnitude class 39  =  0.7231897583187816\n",
      "min magnitude class 39  =  -0.8300542413836668\n",
      "\n",
      " cls 40  >> \n",
      "max magnitude class 40  =  4.487228795878568\n",
      "min magnitude class 40  =  -2.679298218354628\n",
      "after normalizing >>\n",
      "max magnitude class 40  =  0.9501929299684995\n",
      "min magnitude class 40  =  -0.41748678700387554\n",
      "\n",
      " cls 41  >> \n",
      "max magnitude class 41  =  4.437439106446019\n",
      "min magnitude class 41  =  -2.836596938851063\n",
      "after normalizing >>\n",
      "max magnitude class 41  =  0.9406909287598233\n",
      "min magnitude class 41  =  -0.44750610725501117\n",
      "\n",
      " cls 42  >> \n",
      "max magnitude class 42  =  3.670861638132203\n",
      "min magnitude class 42  =  -2.5872513111134197\n",
      "after normalizing >>\n",
      "max magnitude class 42  =  0.7943951773004929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min magnitude class 42  =  -0.39992030217882557\n",
      "each class and its windows =\n",
      "1 48\t2 39\t3 42\t4 45\t5 48\t6 50\t7 57\t8 41\t9 42\t10 36\t11 39\t12 51\t13 51\t14 47\t15 58\t16 56\t17 33\t18 46\t19 48\t20 41\t21 46\t22 46\t23 52\t24 46\t25 49\t26 41\t27 42\t28 45\t29 49\t30 46\t31 51\t32 49\t33 48\t34 42\t35 49\t36 51\t37 50\t38 50\t39 47\t40 46\t41 54\t42 48\t\n",
      " max instance in classes of test windows = 58\n"
     ]
    }
   ],
   "source": [
    "#print(np.max(wndws_tst))\n",
    "for i in range (1,cls_num+1):                # Normalization test windows\n",
    "    cls=i\n",
    "    print('\\n cls', i, ' >> ')\n",
    "    print('max magnitude class', i , ' = ' ,np.max(vars()['wndws_tst'+str(cls)][:,:-1]))\n",
    "    print('min magnitude class', i , ' = ' ,np.min(vars()['wndws_tst'+str(cls)][:,:-1]))\n",
    "\n",
    "    vars()['wndws_tst'+str(i)][:,:-1]= 2*(vars()['wndws_tst'+str(i)][:,:-1]-mn_aug)/(mx_aug - mn_aug) - 1\n",
    "    \n",
    "    print('after normalizing >>')\n",
    "    print('max magnitude class', i , ' = ' ,np.max(vars()['wndws_tst'+str(cls)][:,:-1]))\n",
    "    print('min magnitude class', i , ' = ' ,np.min(vars()['wndws_tst'+str(cls)][:,:-1]))\n",
    "    \n",
    "print(\"each class and its windows =\")                    #تجمیع کلاس های تست\n",
    "cls_wndws=np.array([])\n",
    "for cls in range (1,cls_num+1):                                # آرایه ی تعداد پنجره ی هر کلاس\n",
    "    wns=len(vars()['wndws_tst'+str(cls)])\n",
    "    cls_wndws=np.append(cls_wndws,wns)\n",
    "    print(cls, wns, end='\\t')\n",
    "    wndws_test=np.append(wndws_test,vars()['wndws_tst'+str(cls)],axis=0)\n",
    "    \n",
    "mx_wndws=int(np.max(cls_wndws))                        # حداکثر تعداد پنجره ی موجود بین کلاس ها\n",
    "print('\\n max instance in classes of test windows =', mx_wndws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min tst = -1.0014654308353825\n",
      "max tst = 1.1580859210715708\n"
     ]
    }
   ],
   "source": [
    "print('min tst =', np.min(wndws_test[:,:-1]))\n",
    "print('max tst =', np.max(wndws_test[:,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1965"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wndws_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1965, 481)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(wndws_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=wndws_test[:,:-1]\n",
    "ytest=np.int16(wndws_test[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,\n",
      "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "        2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "        3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "        3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,\n",
      "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "        4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "        5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
      "        7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,\n",
      "        9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "        9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "        9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12,\n",
      "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
      "       13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "       13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "       13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "       17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19,\n",
      "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22,\n",
      "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "       22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "       22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "       23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n",
      "       23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "       24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "       24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
      "       24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "       26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
      "       26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "       27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28,\n",
      "       28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "       28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "       28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29,\n",
      "       29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "       29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "       29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "       30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "       30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "       30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
      "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
      "       31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
      "       31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "       32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "       32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "       32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "       33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "       33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34,\n",
      "       34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "       34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "       34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35,\n",
      "       35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35,\n",
      "       35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35,\n",
      "       35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
      "       36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
      "       36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
      "       36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
      "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
      "       37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
      "       37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
      "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
      "       38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
      "       38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39,\n",
      "       39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39,\n",
      "       39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40,\n",
      "       40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n",
      "       40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40,\n",
      "       40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41,\n",
      "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
      "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
      "       41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42,\n",
      "       42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,\n",
      "       42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,\n",
      "       42, 42, 42, 42, 42, 42, 42, 42, 42, 42], dtype=int16)\n"
     ]
    }
   ],
   "source": [
    "fullprint(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center><div style=\"direction:rtl;font-family:B Nazanin\">Train Windows</div></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nytestt=np.int32(np.array([ytest]).T)\\ntestx=np.concatenate((Xtest, ytestt), axis=1)\\ntestx=np.random.permutation(testx)\\nXtest=np.int32(testx[:,:-1])\\nytest=np.int32(testx[:,-1])'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ytraint=np.int32(np.array([ytrain]).T)\n",
    "#trainx=np.concatenate((Xtrain, ytraint), axis=1)\n",
    "trainx=np.random.permutation(xtrain)\n",
    "Xtrain=np.array(trainx[:,:-1])\n",
    "ytrain=np.int16(trainx[:,-1])\n",
    "'''\n",
    "ytestt=np.int32(np.array([ytest]).T)\n",
    "testx=np.concatenate((Xtest, ytestt), axis=1)\n",
    "testx=np.random.permutation(testx)\n",
    "Xtest=np.int32(testx[:,:-1])\n",
    "ytest=np.int32(testx[:,-1])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 481)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(wndws1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center><div style=\"direction:rtl;font-family:B Nazanin\">Validation Windows</div></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xvalid=np.array(Xtrain)\n",
    "yvalid=np.array(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><div style=\"direction:rtl;font-family:B Nazanin\">CNN And UnBalanced RAW Data</div></center></h1>\n",
    "<h1><center><div style=\"direction:rtl;font-family:Arial\">Cross Entropy Loss Function</div></center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train =>  (1800, 480)\n",
      "y_train =>  (1800,)\n",
      "X_test  =>  (1965, 480)\n",
      "y_test  =>  (1965,)\n",
      "X_valid  =>  (1800, 480)\n",
      "y_valid  =>  (1800,)\n"
     ]
    }
   ],
   "source": [
    "X_train=np.array(Xtrain)\n",
    "y_train=np.array(ytrain)\n",
    "X_test=np.array(Xtest)\n",
    "y_test=np.array(ytest)\n",
    "X_valid=np.array(Xtrain)\n",
    "y_valid=np.array(ytrain)\n",
    "print('X_train => ', X_train.shape)\n",
    "print('y_train => ', y_train.shape)\n",
    "print('X_test  => ', X_test.shape)\n",
    "print('y_test  => ', y_test.shape)\n",
    "print('X_valid  => ', X_valid.shape)\n",
    "print('y_valid  => ', y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=np.array(ytrain)\n",
    "np.min(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat=to_categorical(y_train)\n",
    "y_valid_cat=to_categorical(y_valid)\n",
    "y_test_cat=to_categorical(y_test)\n",
    "\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_valid = np.expand_dims(X_valid, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "#y_train=np.transpose([y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "23/23 [==============================] - 5s 57ms/step - loss: 4.1059 - accuracy: 0.0372 - val_loss: 3.7546 - val_accuracy: 0.0250\n",
      "Epoch 2/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 3.0439 - accuracy: 0.1328 - val_loss: 3.7424 - val_accuracy: 0.0339\n",
      "Epoch 3/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 2.4858 - accuracy: 0.2467 - val_loss: 3.7132 - val_accuracy: 0.0783\n",
      "Epoch 4/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 1.9771 - accuracy: 0.3578 - val_loss: 3.7087 - val_accuracy: 0.0672\n",
      "Epoch 5/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 1.6524 - accuracy: 0.4489 - val_loss: 3.6884 - val_accuracy: 0.0611\n",
      "Epoch 6/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 1.3501 - accuracy: 0.5461 - val_loss: 3.7149 - val_accuracy: 0.0289\n",
      "Epoch 7/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 1.1363 - accuracy: 0.6200 - val_loss: 3.7074 - val_accuracy: 0.0472\n",
      "Epoch 8/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.9358 - accuracy: 0.6744 - val_loss: 3.6733 - val_accuracy: 0.0294\n",
      "Epoch 9/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.8591 - accuracy: 0.7000 - val_loss: 4.2169 - val_accuracy: 0.0289\n",
      "Epoch 10/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.7934 - accuracy: 0.7289 - val_loss: 4.0951 - val_accuracy: 0.0450\n",
      "Epoch 11/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.6469 - accuracy: 0.7633 - val_loss: 4.8697 - val_accuracy: 0.0289\n",
      "Epoch 12/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.6874 - accuracy: 0.7600 - val_loss: 4.2475 - val_accuracy: 0.0289\n",
      "Epoch 13/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.5078 - accuracy: 0.8167 - val_loss: 4.6920 - val_accuracy: 0.0417\n",
      "Epoch 14/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.5282 - accuracy: 0.8172 - val_loss: 4.6568 - val_accuracy: 0.0289\n",
      "Epoch 15/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.5297 - accuracy: 0.8161 - val_loss: 4.8205 - val_accuracy: 0.0906\n",
      "Epoch 16/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.4256 - accuracy: 0.8450 - val_loss: 4.0029 - val_accuracy: 0.1161\n",
      "Epoch 17/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.3974 - accuracy: 0.8556 - val_loss: 3.3960 - val_accuracy: 0.1550\n",
      "Epoch 18/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.4304 - accuracy: 0.8472 - val_loss: 4.6280 - val_accuracy: 0.1217\n",
      "Epoch 19/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.3451 - accuracy: 0.8650 - val_loss: 3.4083 - val_accuracy: 0.1750\n",
      "Epoch 20/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.3510 - accuracy: 0.8717 - val_loss: 4.3689 - val_accuracy: 0.1728\n",
      "Epoch 21/1000\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.3249 - accuracy: 0.8772 - val_loss: 2.5932 - val_accuracy: 0.2600\n",
      "Epoch 22/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.3635 - accuracy: 0.8817 - val_loss: 2.3794 - val_accuracy: 0.2983\n",
      "Epoch 23/1000\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.2925 - accuracy: 0.8972 - val_loss: 2.6311 - val_accuracy: 0.3944\n",
      "Epoch 24/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.3398 - accuracy: 0.8761 - val_loss: 1.2763 - val_accuracy: 0.5956\n",
      "Epoch 25/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.2256 - accuracy: 0.9172 - val_loss: 0.8317 - val_accuracy: 0.7506\n",
      "Epoch 26/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.3030 - accuracy: 0.8989 - val_loss: 0.6295 - val_accuracy: 0.7844\n",
      "Epoch 27/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.2263 - accuracy: 0.9206 - val_loss: 0.4729 - val_accuracy: 0.8411\n",
      "Epoch 28/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.2538 - accuracy: 0.9078 - val_loss: 0.3787 - val_accuracy: 0.8633\n",
      "Epoch 29/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.2381 - accuracy: 0.9139 - val_loss: 0.3621 - val_accuracy: 0.8628\n",
      "Epoch 30/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.2314 - accuracy: 0.9172 - val_loss: 0.3142 - val_accuracy: 0.8878\n",
      "Epoch 31/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1799 - accuracy: 0.9361 - val_loss: 0.8039 - val_accuracy: 0.7722\n",
      "Epoch 32/1000\n",
      "23/23 [==============================] - 1s 27ms/step - loss: 0.2634 - accuracy: 0.9111 - val_loss: 0.3897 - val_accuracy: 0.8722\n",
      "Epoch 33/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1486 - accuracy: 0.9461 - val_loss: 1.2991 - val_accuracy: 0.7433\n",
      "Epoch 34/1000\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.2121 - accuracy: 0.9406 - val_loss: 0.2190 - val_accuracy: 0.9172\n",
      "Epoch 35/1000\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.1959 - accuracy: 0.9222 - val_loss: 0.1450 - val_accuracy: 0.9544\n",
      "Epoch 36/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1649 - accuracy: 0.9478 - val_loss: 0.1319 - val_accuracy: 0.9544\n",
      "Epoch 37/1000\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.1573 - accuracy: 0.9428 - val_loss: 0.0866 - val_accuracy: 0.9644\n",
      "Epoch 38/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1377 - accuracy: 0.9506 - val_loss: 0.0989 - val_accuracy: 0.9550\n",
      "Epoch 39/1000\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.1428 - accuracy: 0.9561 - val_loss: 0.0707 - val_accuracy: 0.9706\n",
      "Epoch 40/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.1419 - accuracy: 0.9556 - val_loss: 0.0786 - val_accuracy: 0.9733\n",
      "Epoch 41/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.1575 - accuracy: 0.9500 - val_loss: 0.0787 - val_accuracy: 0.9678\n",
      "Epoch 42/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0903 - accuracy: 0.9667 - val_loss: 0.2351 - val_accuracy: 0.9333\n",
      "Epoch 43/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1380 - accuracy: 0.9589 - val_loss: 0.1452 - val_accuracy: 0.9600\n",
      "Epoch 44/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1348 - accuracy: 0.9544 - val_loss: 0.0591 - val_accuracy: 0.9789\n",
      "Epoch 45/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1186 - accuracy: 0.9650 - val_loss: 0.1436 - val_accuracy: 0.9606\n",
      "Epoch 46/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.2121 - accuracy: 0.9428 - val_loss: 0.0745 - val_accuracy: 0.9700\n",
      "Epoch 47/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0816 - accuracy: 0.9722 - val_loss: 0.1903 - val_accuracy: 0.9539\n",
      "Epoch 48/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1215 - accuracy: 0.9644 - val_loss: 0.0562 - val_accuracy: 0.9811\n",
      "Epoch 49/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1433 - accuracy: 0.9661 - val_loss: 0.4273 - val_accuracy: 0.8528\n",
      "Epoch 50/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0901 - accuracy: 0.9778 - val_loss: 0.2431 - val_accuracy: 0.9506\n",
      "Epoch 51/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1511 - accuracy: 0.9561 - val_loss: 0.0465 - val_accuracy: 0.9861\n",
      "Epoch 52/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0722 - accuracy: 0.9778 - val_loss: 0.1297 - val_accuracy: 0.9639\n",
      "Epoch 53/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1379 - accuracy: 0.9628 - val_loss: 0.0851 - val_accuracy: 0.9844\n",
      "Epoch 54/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1405 - accuracy: 0.9572 - val_loss: 0.0081 - val_accuracy: 0.9994\n",
      "Epoch 55/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1011 - accuracy: 0.9822 - val_loss: 0.0330 - val_accuracy: 0.9856\n",
      "Epoch 56/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1197 - accuracy: 0.9683 - val_loss: 0.1986 - val_accuracy: 0.9572\n",
      "Epoch 57/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0802 - accuracy: 0.9772 - val_loss: 0.1116 - val_accuracy: 0.9656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0874 - accuracy: 0.9739 - val_loss: 0.0854 - val_accuracy: 0.9706\n",
      "Epoch 59/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1277 - accuracy: 0.9733 - val_loss: 0.1227 - val_accuracy: 0.9661\n",
      "Epoch 60/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0732 - accuracy: 0.9778 - val_loss: 0.3691 - val_accuracy: 0.9294\n",
      "Epoch 61/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1003 - accuracy: 0.9767 - val_loss: 0.0787 - val_accuracy: 0.9728\n",
      "Epoch 62/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1039 - accuracy: 0.9794 - val_loss: 0.0367 - val_accuracy: 0.9872\n",
      "Epoch 63/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0855 - accuracy: 0.9789 - val_loss: 0.5262 - val_accuracy: 0.8500\n",
      "Epoch 64/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0635 - accuracy: 0.9806 - val_loss: 0.0357 - val_accuracy: 0.9861\n",
      "Epoch 65/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.2036 - accuracy: 0.9578 - val_loss: 0.0324 - val_accuracy: 0.9861\n",
      "Epoch 66/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0184 - accuracy: 0.9928 - val_loss: 0.3452 - val_accuracy: 0.9378\n",
      "Epoch 67/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1226 - accuracy: 0.9739 - val_loss: 0.0206 - val_accuracy: 0.9900\n",
      "Epoch 68/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1347 - accuracy: 0.9706 - val_loss: 0.0068 - val_accuracy: 0.9978\n",
      "Epoch 69/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.2960 - accuracy: 0.9628 - val_loss: 0.0610 - val_accuracy: 0.9789\n",
      "Epoch 70/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0208 - accuracy: 0.9911 - val_loss: 0.2880 - val_accuracy: 0.9467\n",
      "Epoch 71/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1498 - accuracy: 0.9689 - val_loss: 0.0065 - val_accuracy: 0.9983\n",
      "Epoch 72/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.0350 - accuracy: 0.9922 - val_loss: 0.2938 - val_accuracy: 0.8994\n",
      "Epoch 73/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1576 - accuracy: 0.9622 - val_loss: 0.0035 - val_accuracy: 0.9989\n",
      "Epoch 74/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.0605 - accuracy: 0.9872 - val_loss: 0.0040 - val_accuracy: 0.9989\n",
      "Epoch 75/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1371 - accuracy: 0.9728 - val_loss: 0.1383 - val_accuracy: 0.9706\n",
      "Epoch 76/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.0789 - accuracy: 0.9822 - val_loss: 0.0335 - val_accuracy: 0.9894\n",
      "Epoch 77/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1171 - accuracy: 0.9800 - val_loss: 0.0465 - val_accuracy: 0.9850\n",
      "Epoch 78/1000\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.0080 - accuracy: 0.9978 - val_loss: 0.0024 - val_accuracy: 0.9989\n",
      "Epoch 79/1000\n",
      "23/23 [==============================] - 1s 26ms/step - loss: 0.1610 - accuracy: 0.9711 - val_loss: 0.0361 - val_accuracy: 0.9900\n",
      "Epoch 80/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0866 - accuracy: 0.9778 - val_loss: 0.0230 - val_accuracy: 0.9967\n",
      "Epoch 81/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.0963 - accuracy: 0.9839 - val_loss: 0.1142 - val_accuracy: 0.9722\n",
      "Epoch 82/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0849 - accuracy: 0.9872 - val_loss: 0.0366 - val_accuracy: 0.9900\n",
      "Epoch 83/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1094 - accuracy: 0.9778 - val_loss: 0.5058 - val_accuracy: 0.9394\n",
      "Epoch 84/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0971 - accuracy: 0.9778 - val_loss: 0.1688 - val_accuracy: 0.9344\n",
      "Epoch 85/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0680 - accuracy: 0.9772 - val_loss: 0.0977 - val_accuracy: 0.9794\n",
      "Epoch 86/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0828 - accuracy: 0.9783 - val_loss: 0.0168 - val_accuracy: 0.9950\n",
      "Epoch 87/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1321 - accuracy: 0.9739 - val_loss: 0.0204 - val_accuracy: 0.9950\n",
      "Epoch 88/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0968 - accuracy: 0.9822 - val_loss: 0.0222 - val_accuracy: 0.9922\n",
      "Epoch 89/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0447 - accuracy: 0.9878 - val_loss: 8.8819e-04 - val_accuracy: 1.0000\n",
      "Epoch 90/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1139 - accuracy: 0.9778 - val_loss: 0.0092 - val_accuracy: 0.9978\n",
      "Epoch 91/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 0.0584 - val_accuracy: 0.9933\n",
      "Epoch 92/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1142 - accuracy: 0.9767 - val_loss: 0.0836 - val_accuracy: 0.9744\n",
      "Epoch 93/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0104 - accuracy: 0.9967 - val_loss: 2.0359e-04 - val_accuracy: 1.0000\n",
      "Epoch 94/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 7.2138e-05 - accuracy: 1.0000 - val_loss: 9.7004e-06 - val_accuracy: 1.0000\n",
      "Epoch 95/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 2.3975e-05 - accuracy: 1.0000 - val_loss: 0.1105 - val_accuracy: 0.9933\n",
      "Epoch 96/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.2066 - accuracy: 0.9661 - val_loss: 0.0080 - val_accuracy: 0.9972\n",
      "Epoch 97/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0373 - accuracy: 0.9894 - val_loss: 0.5579 - val_accuracy: 0.9500\n",
      "Epoch 98/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1358 - accuracy: 0.9694 - val_loss: 0.0326 - val_accuracy: 0.9878\n",
      "Epoch 99/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0439 - accuracy: 0.9894 - val_loss: 0.3139 - val_accuracy: 0.9533\n",
      "Epoch 100/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1189 - accuracy: 0.9789 - val_loss: 0.0051 - val_accuracy: 0.9983\n",
      "Epoch 101/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0076 - accuracy: 0.9972 - val_loss: 0.0011 - val_accuracy: 0.9994\n",
      "Epoch 102/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0901 - accuracy: 0.9811 - val_loss: 0.0600 - val_accuracy: 0.9817\n",
      "Epoch 103/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1096 - accuracy: 0.9839 - val_loss: 0.0170 - val_accuracy: 0.9944\n",
      "Epoch 104/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0910 - accuracy: 0.9811 - val_loss: 0.0121 - val_accuracy: 0.9961\n",
      "Epoch 105/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0247 - accuracy: 0.9939 - val_loss: 0.4147 - val_accuracy: 0.9261\n",
      "Epoch 106/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0743 - accuracy: 0.9800 - val_loss: 1.5775 - val_accuracy: 0.8833\n",
      "Epoch 107/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1562 - accuracy: 0.9789 - val_loss: 0.0040 - val_accuracy: 0.9989\n",
      "Epoch 108/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0288 - accuracy: 0.9922 - val_loss: 0.0288 - val_accuracy: 0.9911\n",
      "Epoch 109/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0904 - accuracy: 0.9789 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "Epoch 110/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0295 - accuracy: 0.9967 - val_loss: 0.5038 - val_accuracy: 0.9233\n",
      "Epoch 111/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.1492 - accuracy: 0.9761 - val_loss: 0.2427 - val_accuracy: 0.9217\n",
      "Epoch 112/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0563 - accuracy: 0.9867 - val_loss: 0.0041 - val_accuracy: 0.9983\n",
      "Epoch 113/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.0845 - accuracy: 0.9783 - val_loss: 0.1249 - val_accuracy: 0.9639\n",
      "Epoch 114/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.0590 - accuracy: 0.9867 - val_loss: 0.2689 - val_accuracy: 0.9456\n",
      "Epoch 115/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0404 - accuracy: 0.9917 - val_loss: 3.0980e-04 - val_accuracy: 1.0000\n",
      "Epoch 116/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 7.4573e-05 - accuracy: 1.0000 - val_loss: 8.7029e-05 - val_accuracy: 1.0000\n",
      "Epoch 117/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 2.2074e-05 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 0.9967\n",
      "Epoch 118/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.1742 - accuracy: 0.9650 - val_loss: 0.0133 - val_accuracy: 0.9972\n",
      "Epoch 119/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0436 - accuracy: 0.9889 - val_loss: 0.0251 - val_accuracy: 0.9911\n",
      "Epoch 120/1000\n",
      "23/23 [==============================] - 1s 25ms/step - loss: 0.0675 - accuracy: 0.9872 - val_loss: 0.0280 - val_accuracy: 0.9906\n",
      "Epoch 121/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0810 - accuracy: 0.9811 - val_loss: 0.0308 - val_accuracy: 0.9900\n",
      "Epoch 122/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0170 - accuracy: 0.9961 - val_loss: 0.0143 - val_accuracy: 0.9972\n",
      "Epoch 123/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1152 - accuracy: 0.9833 - val_loss: 0.0028 - val_accuracy: 0.9994\n",
      "Epoch 124/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0154 - accuracy: 0.9972 - val_loss: 0.0672 - val_accuracy: 0.9894\n",
      "Epoch 125/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0986 - accuracy: 0.9822 - val_loss: 0.0017 - val_accuracy: 0.9994\n",
      "Epoch 126/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.1158 - accuracy: 0.9850 - val_loss: 0.0321 - val_accuracy: 0.9933\n",
      "Epoch 127/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0646 - accuracy: 0.9922 - val_loss: 1.1612 - val_accuracy: 0.8994\n",
      "Epoch 128/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1036 - accuracy: 0.9867 - val_loss: 3.7501e-04 - val_accuracy: 1.0000\n",
      "Epoch 129/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0778 - accuracy: 0.9844 - val_loss: 0.0089 - val_accuracy: 0.9978\n",
      "Epoch 130/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0879 - accuracy: 0.9806 - val_loss: 0.1880 - val_accuracy: 0.9694\n",
      "Epoch 131/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0682 - accuracy: 0.9861 - val_loss: 0.0074 - val_accuracy: 0.9972\n",
      "Epoch 132/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 8.4247e-05 - val_accuracy: 1.0000\n",
      "Epoch 133/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 5.7230e-05 - accuracy: 1.0000 - val_loss: 2.1868e-05 - val_accuracy: 1.0000\n",
      "Epoch 134/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1976 - accuracy: 0.9822 - val_loss: 0.0338 - val_accuracy: 0.9883\n",
      "Epoch 135/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.1211 - val_accuracy: 0.9817\n",
      "Epoch 136/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1319 - accuracy: 0.9839 - val_loss: 0.0463 - val_accuracy: 0.9911\n",
      "Epoch 137/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.0062 - accuracy: 0.9978 - val_loss: 1.0625 - val_accuracy: 0.9244\n",
      "Epoch 138/1000\n",
      "23/23 [==============================] - 1s 23ms/step - loss: 0.1661 - accuracy: 0.9744 - val_loss: 0.0792 - val_accuracy: 0.9828\n",
      "Epoch 139/1000\n",
      "23/23 [==============================] - 1s 24ms/step - loss: 0.0206 - accuracy: 0.9950 - val_loss: 0.3732 - val_accuracy: 0.9744\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 8.8819e-04 - accuracy: 1.0000\n",
      "np.shape(X_test)= (1965, 480, 1)\n",
      "Accuracy=  1.0\n"
     ]
    }
   ],
   "source": [
    "#VGG16_1D\n",
    "\n",
    "accuracy=0\n",
    "acc_crs=np.array([])\n",
    "##for i in range (10):\n",
    "##    if accuracy<0.2 :\n",
    "\n",
    "verbose, epochs, batch_size = 1, 1000, 80\n",
    "n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train_cat.shape[1]\n",
    "steps_per_epoch = len(X_train)//batch_size\n",
    "validation_steps = len(X_valid)//batch_size # if you have test data\n",
    "\n",
    "\n",
    "\n",
    "model_crs = Sequential()\n",
    "#model_crs.add(Conv1D(input_shape=x_train.shape[1:],filters=64,kernel_size=9,padding=\"same\", activation=\"relu\"))\n",
    "model_crs.add(Conv1D(filters=128, kernel_size=12, strides=1, activation='relu', input_shape=(n_timesteps,n_features))) #Replaced\n",
    "model_crs.add(BatchNormalization())\n",
    "model_crs.add(MaxPooling1D(pool_size=2,strides=3))\n",
    "\n",
    "model_crs.add(Conv1D(filters=32, kernel_size=7, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model_crs.add(BatchNormalization())\n",
    "model_crs.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "\n",
    "model_crs.add(Conv1D(filters=32, kernel_size=10, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model_crs.add(Conv1D(filters=128, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\"))\n",
    "model_crs.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "\n",
    "model_crs.add(Conv1D(filters=256, kernel_size=15, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model_crs.add(MaxPooling1D(pool_size=2,strides=2))\n",
    "\n",
    "model_crs.add(Conv1D(filters=512, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "#model_crs.add(Conv1D(filters=128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "\n",
    "model_crs.add(Flatten())\n",
    "model_crs.add(Dropout(0.1))\n",
    "model_crs.add(Dense(units=512,activation=\"relu\"))\n",
    "#model_crs.add(Dense(units=2048,activation=\"relu\"))\n",
    "model_crs.add(Dense(n_outputs, activation='softmax'))   #Replaced here from old Model\n",
    "\n",
    "model_crs.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# fit network   #CategoricalCrossentropy #sparse_categorical_crossentropy #SparseCategoricalCrossentropy\n",
    "\n",
    "earlystopping = callbacks.EarlyStopping(monitor =\"val_accuracy\", mode =\"max\", patience = 50, restore_best_weights = True)\n",
    "\n",
    "model_crs.fit(X_train, y_train_cat, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_data = (X_valid, y_valid_cat), callbacks =[earlystopping])\n",
    "\n",
    "_, accuracy = model_crs.evaluate(X_valid, y_valid_cat, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('np.shape(X_test)=',np.shape(X_test))\n",
    "\n",
    "print('Accuracy= ', accuracy)\n",
    "\n",
    "#y_pred_vgg = model_crs.predict_classes(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxindx(ar):\n",
    "    ar_cpy=np.array(ar)\n",
    "    pred=np.argmax(ar,axis=1)\n",
    "    for i in range (len(pred)):\n",
    "        if pred[i]==0:\n",
    "            ar_cpy[i,0]=-1000\n",
    "    pred=np.argmax(ar_cpy,axis=1)\n",
    "    return(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 5ms/step\n",
      "[[38  0  1 ...  0  0  0]\n",
      " [ 0 38  0 ...  0  0  0]\n",
      " [ 2  0 13 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 38  0  0]\n",
      " [ 0  0  0 ...  0 42  0]\n",
      " [ 0  0  0 ...  0  0 48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.79      0.85        48\n",
      "           2       1.00      0.97      0.99        39\n",
      "           3       0.48      0.31      0.38        42\n",
      "           4       0.85      0.49      0.62        45\n",
      "           5       1.00      0.85      0.92        48\n",
      "           6       0.75      0.80      0.78        50\n",
      "           7       0.96      0.96      0.96        57\n",
      "           8       0.84      1.00      0.91        41\n",
      "           9       0.98      0.95      0.96        42\n",
      "          10       0.97      1.00      0.99        36\n",
      "          11       0.83      1.00      0.91        39\n",
      "          12       1.00      1.00      1.00        51\n",
      "          13       0.98      0.88      0.93        51\n",
      "          14       0.80      0.87      0.84        47\n",
      "          15       0.98      0.97      0.97        58\n",
      "          16       0.98      0.93      0.95        56\n",
      "          17       0.45      0.91      0.60        33\n",
      "          18       1.00      1.00      1.00        46\n",
      "          19       1.00      1.00      1.00        48\n",
      "          20       0.89      1.00      0.94        41\n",
      "          21       0.92      0.98      0.95        46\n",
      "          22       0.98      1.00      0.99        46\n",
      "          23       0.92      0.85      0.88        52\n",
      "          24       0.94      1.00      0.97        46\n",
      "          25       1.00      1.00      1.00        49\n",
      "          26       0.98      1.00      0.99        41\n",
      "          27       0.64      0.71      0.67        42\n",
      "          28       1.00      1.00      1.00        45\n",
      "          29       0.91      0.98      0.94        49\n",
      "          30       0.56      0.59      0.57        46\n",
      "          31       0.89      0.96      0.92        51\n",
      "          32       1.00      0.78      0.87        49\n",
      "          33       1.00      0.98      0.99        48\n",
      "          34       0.93      1.00      0.97        42\n",
      "          35       0.98      0.86      0.91        49\n",
      "          36       0.84      0.80      0.82        51\n",
      "          37       0.98      0.98      0.98        50\n",
      "          38       0.96      1.00      0.98        50\n",
      "          39       0.90      0.94      0.92        47\n",
      "          40       0.90      0.83      0.86        46\n",
      "          41       0.88      0.78      0.82        54\n",
      "          42       1.00      1.00      1.00        48\n",
      "\n",
      "    accuracy                           0.90      1965\n",
      "   macro avg       0.90      0.90      0.89      1965\n",
      "weighted avg       0.91      0.90      0.90      1965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_x=model_crs.predict(X_test)              # Function 1\n",
    "\n",
    "y_pred_crs=maxindx(predict_x)                    # function from augment.py to remove 0 index predictions\n",
    "\n",
    "#y_pred_crs = model_crs.predict_classes(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_crs)\n",
    "print(cm)\n",
    "acc3=accuracy_score(y_test, y_pred_crs)\n",
    "\n",
    "print(classification_report(y_test, y_pred_crs))\n",
    "\n",
    "#sns.heatmap(cm, annot=True)\n",
    "\n",
    "#plt.imshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
